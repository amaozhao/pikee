# 可靠性增强方案详解

> LangGraph 工作流的重试、错误处理与兜底策略

---

## 1. 可靠性挑战

### 1.1 常见问题

| 问题 | 影响 | 频率 |
|------|------|------|
| **LLM API 超时** | 流程中断 | 偶发 |
| **向量库连接失败** | 存储失败 | 罕见 |
| **Neo4j 写入冲突** | 数据不一致 | 罕见 |
| **文档解析失败** | 无法处理 | 常见（格式问题）|
| **Atom 提取质量差** | 检索效果差 | 偶发 |

### 1.2 解决方案架构

```
┌─────────────────────────────────────┐
│        LangGraph 节点执行            │
└────────────┬────────────────────────┘
             │
        ┌────▼────┐
        │ 成功？  │
        └─┬────┬──┘
          │    │
      [成功]  [失败]
          │    │
          │    └──→ ┌─────────────────┐
          │         │ 错误分类         │
          │         │ • RetryableError │
          │         │ • FatalError     │
          │         └────┬─────────────┘
          │              │
          │         ┌────▼────┐
          │         │ 可重试？ │
          │         └─┬────┬──┘
          │       [是] [否]
          │          │    │
          │    ┌─────▼──┐ │
          │    │ 重试N次 │ │
          │    │指数退避 │ │
          │    └─┬──────┘ │
          │      │        │
          │ ┌────▼────┐   │
          │ │重试成功？│   │
          │ └─┬────┬──┘   │
          │ [是] [否]     │
          │   │    └──────┼──→ ┌─────────┐
          │   │           │    │ 兜底策略 │
          └───┴───────────┴──→ └─────────┘
```

---

## 2. 核心依赖

```toml
[tool.poetry.dependencies]
tenacity = "*"              # 重试库
python-json-logger = "*"    # 结构化日志
sentry-sdk = "*"           # 错误追踪
redis = "*"                 # 状态持久化
```

---

## 3. 重试机制

### 3.1 重试装饰器

```python
# app/utils/retry.py
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
    after_log
)
import logging
from functools import wraps

logger = logging.getLogger(__name__)

def retry_with_config(
    retry_on: tuple[type[Exception], ...] = (Exception,),
    max_attempts: int = 3,
    wait_multiplier: float = 2.0,
    wait_max: int = 60,
):
    """
    配置化重试装饰器
    
    Args:
        retry_on: 需要重试的异常类型
        max_attempts: 最大重试次数
        wait_multiplier: 指数退避倍数
        wait_max: 最大等待时间（秒）
    
    示例:
        @retry_with_config(
            retry_on=(TimeoutError, ConnectionError),
            max_attempts=5
        )
        async def call_llm():
            ...
    """
    def decorator(func):
        @retry(
            retry=retry_if_exception_type(retry_on),
            stop=stop_after_attempt(max_attempts),
            wait=wait_exponential(
                multiplier=wait_multiplier,
                max=wait_max
            ),
            before_sleep=before_sleep_log(logger, logging.WARNING),
            after=after_log(logger, logging.INFO),
            reraise=True
        )
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            return await func(*args, **kwargs)
        
        @retry(
            retry=retry_if_exception_type(retry_on),
            stop=stop_after_attempt(max_attempts),
            wait=wait_exponential(
                multiplier=wait_multiplier,
                max=wait_max
            ),
            before_sleep=before_sleep_log(logger, logging.WARNING),
            after=after_log(logger, logging.INFO),
            reraise=True
        )
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        import asyncio
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator
```

### 3.2 使用示例

```python
from app.utils.retry import retry_with_config

# LLM 调用重试
@retry_with_config(
    retry_on=(TimeoutError, ConnectionError),
    max_attempts=3
)
async def call_llm(prompt: str) -> str:
    response = await llm.ainvoke(prompt)
    return response.content

# Qdrant 写入重试
@retry_with_config(
    retry_on=(ConnectionError,),
    max_attempts=5,
    wait_max=120
)
async def save_to_qdrant(vectors: list):
    await qdrant_client.upsert(...)
```

---

## 4. 错误分类

### 4.1 自定义异常类型

```python
# app/utils/exceptions.py

class RetryableError(Exception):
    """可重试的错误（网络、超时等）"""
    pass

class FatalError(Exception):
    """致命错误（不可重试，如文件不存在）"""
    pass

class PartialSuccessError(Exception):
    """部分成功（可继续后续流程）"""
    def __init__(self, message: str, partial_data: dict):
        super().__init__(message)
        self.partial_data = partial_data
```

### 4.2 错误分类策略

```python
def classify_error(error: Exception) -> str:
    """错误分类"""
    
    # 网络相关 → 可重试
    if isinstance(error, (ConnectionError, TimeoutError)):
        return "retryable"
    
    # 文件不存在 → 致命
    if isinstance(error, FileNotFoundError):
        return "fatal"
    
    # LLM API 限流 → 可重试
    if "rate_limit" in str(error).lower():
        return "retryable"
    
    # 数据验证错误 → 致命
    if isinstance(error, ValueError):
        return "fatal"
    
    # 默认可重试
    return "retryable"
```

---

## 5. 状态持久化

### 5.1 状态管理器

```python
# app/services/state_manager.py
import redis.asyncio as redis
import json
from datetime import timedelta

class StateManager:
    """Pipeline 状态持久化（Redis）"""
    
    def __init__(self):
        self.client = redis.from_url(
            settings.redis_url,
            decode_responses=True
        )
        self.ttl = timedelta(days=7)
    
    async def save_state(self, task_id: str, state: dict):
        """保存状态快照"""
        key = f"pipeline:state:{task_id}"
        await self.client.setex(
            key,
            self.ttl,
            json.dumps(state, default=str)
        )
    
    async def get_state(self, task_id: str) -> dict | None:
        """获取状态"""
        key = f"pipeline:state:{task_id}"
        data = await self.client.get(key)
        return json.loads(data) if data else None
    
    async def update_state(self, task_id: str, updates: dict):
        """更新部分状态"""
        current = await self.get_state(task_id)
        if current:
            current.update(updates)
            await self.save_state(task_id, current)
```

### 5.2 在 LangGraph 中使用

```python
async def wrapped_node(state: PipelineState) -> dict:
    """包装节点，自动保存状态"""
    
    # 执行节点逻辑
    result = await node_func(state)
    
    # 保存状态快照
    await state_manager.save_state(
        task_id=state["task_id"],
        state={**state, **result}
    )
    
    return result
```

---

## 6. LangGraph 增强实现

### 6.1 增强的工作流

```python
# app/pipelines/knowledge_building_enhanced.py
from langgraph.graph import StateGraph, END

class EnhancedPipeline:
    """增强的知识构建 Pipeline"""
    
    def _build_workflow(self):
        workflow = StateGraph(PipelineState)
        
        # 添加节点（每个节点自动包装）
        workflow.add_node("load_doc", self._wrap(self._load_doc))
        workflow.add_node("chunk", self._wrap(self._chunk))
        workflow.add_node("extract_atoms", self._wrap(self._extract_atoms))
        workflow.add_node("create_graph", self._wrap(self._create_graph))
        workflow.add_node("handle_error", self._handle_error)
        
        # 条件边（根据状态决定流向）
        workflow.add_conditional_edges(
            "load_doc",
            self._should_continue,
            {
                "continue": "chunk",
                "error": "handle_error",
                "end": END
            }
        )
        
        workflow.add_conditional_edges(
            "chunk",
            self._should_continue,
            {
                "continue": "extract_atoms",
                "error": "handle_error"
            }
        )
        
        # ... 其他边
        
        return workflow.compile()
    
    def _wrap(self, node_func):
        """节点包装器：重试 + 错误处理 + 状态保存"""
        
        @retry_with_config(
            retry_on=(RetryableError, ConnectionError, TimeoutError),
            max_attempts=settings.max_retry_attempts
        )
        async def wrapped(state: PipelineState):
            node_name = node_func.__name__
            logger.info(f"[{state['task_id']}] Executing: {node_name}")
            
            try:
                # 执行节点
                result = await node_func(state)
                
                # 保存状态
                await state_manager.save_state(
                    state["task_id"],
                    {**state, **result}
                )
                
                return result
            
            except FatalError as e:
                # 致命错误，不重试
                logger.error(f"Fatal error: {e}")
                return {
                    "status": "failed",
                    "error": str(e),
                    "error_type": "fatal"
                }
            
            except (RetryableError, ConnectionError, TimeoutError) as e:
                # 可重试，记录重试次数
                state["retry_count"] = state.get("retry_count", 0) + 1
                logger.warning(f"Retryable error: {e}")
                raise  # 让装饰器处理重试
            
            except Exception as e:
                # 未知错误，记录并上报
                logger.error(f"Unknown error: {e}")
                await error_handler.log_error(
                    task_id=state["task_id"],
                    node=node_name,
                    error=e
                )
                return {
                    "status": "failed",
                    "error": str(e),
                    "error_type": "unknown"
                }
        
        return wrapped
    
    def _should_continue(self, state: PipelineState) -> str:
        """判断是否继续"""
        if state.get("status") == "failed":
            retry_count = state.get("retry_count", 0)
            if retry_count < settings.max_retry_attempts:
                return "error"  # 进入错误处理
            else:
                return "end"  # 达到最大重试，终止
        return "continue"
```

---

## 7. 兜底策略

### 7.1 兜底节点

```python
async def _handle_error(self, state: PipelineState) -> dict:
    """错误处理节点"""
    
    # 1. 记录错误
    await error_handler.log_error(
        task_id=state["task_id"],
        node=state.get("current_node"),
        error=state.get("error")
    )
    
    # 2. 尝试兜底策略
    fallback_result = await self._try_fallback(state)
    
    if fallback_result:
        logger.info(f"Fallback succeeded")
        return {
            "status": "partial_success",
            **fallback_result
        }
    else:
        return {"status": "failed"}
```

### 7.2 具体兜底方案

```python
async def _try_fallback(self, state: PipelineState) -> dict | None:
    """根据失败节点采取不同兜底策略"""
    
    node = state.get("current_node")
    
    # Atom 提取失败 → 使用规则提取
    if node == "extract_atoms":
        logger.info("Fallback: rule-based atom extraction")
        chunks = state["chunks"]
        for chunk in chunks:
            # 简单规则：提取问句
            atoms = extract_questions_by_regex(chunk["content"])
            chunk["atoms"] = atoms
        return {"chunks_with_atoms": chunks}
    
    # 图谱创建失败 → 仅存向量
    elif node == "create_graph":
        logger.info("Fallback: skip graph, vectors only")
        return {"skip_graph": True}
    
    # 向量存储失败 → 降级到仅图谱
    elif node == "store_vectors":
        logger.info("Fallback: skip vectors, graph only")
        return {"skip_vectors": True}
    
    return None

def extract_questions_by_regex(text: str) -> list[str]:
    """规则提取问题（兜底方案）"""
    import re
    questions = re.findall(r'[^。！？]+[？\?]', text)
    return questions[:5]  # 最多 5 个
```

---

## 8. 监控与告警

### 8.1 错误处理器

```python
# app/services/error_handler.py
import sentry_sdk

class ErrorHandler:
    """统一错误处理"""
    
    def __init__(self):
        if settings.enable_sentry:
            sentry_sdk.init(
                dsn=settings.sentry_dsn,
                environment=settings.env
            )
    
    async def log_error(
        self,
        task_id: str,
        node: str,
        error: Exception
    ):
        """记录错误"""
        error_info = {
            "task_id": task_id,
            "node": node,
            "error": str(error),
            "type": type(error).__name__
        }
        
        # 结构化日志
        logger.error("Pipeline error", extra=error_info)
        
        # Sentry 上报
        if settings.enable_sentry:
            with sentry_sdk.push_scope() as scope:
                scope.set_context("pipeline", error_info)
                sentry_sdk.capture_exception(error)
```

### 8.2 Prometheus 指标

```python
from prometheus_client import Counter, Histogram

# 定义指标
pipeline_errors = Counter(
    'pipeline_errors_total',
    'Total pipeline errors',
    ['node', 'error_type']
)

pipeline_retries = Counter(
    'pipeline_retries_total',
    'Total retry attempts',
    ['node']
)

# 在代码中使用
pipeline_errors.labels(node="extract_atoms", error_type="timeout").inc()
pipeline_retries.labels(node="create_graph").inc()
```

---

## 9. 测试示例

```python
# tests/test_reliability.py
import pytest

@pytest.mark.asyncio
async def test_retry_mechanism():
    """测试重试机制"""
    call_count = 0
    
    @retry_with_config(max_attempts=3)
    async def flaky_function():
        nonlocal call_count
        call_count += 1
        if call_count < 3:
            raise ConnectionError("Temporary failure")
        return "success"
    
    result = await flaky_function()
    assert result == "success"
    assert call_count == 3

@pytest.mark.asyncio
async def test_fallback_strategy():
    """测试兜底策略"""
    pipeline = EnhancedPipeline()
    
    # 模拟 Atom 提取失败
    state = {
        "task_id": "test",
        "current_node": "extract_atoms",
        "chunks": [{"content": "这是什么？"}]
    }
    
    result = await pipeline._try_fallback(state)
    assert result is not None
    assert "chunks_with_atoms" in result
```

---

## 10. 最佳实践

### 10.1 重试配置建议

| 场景 | max_attempts | wait_multiplier | wait_max |
|------|--------------|-----------------|----------|
| **LLM API** | 3 | 2.0 | 60s |
| **数据库写入** | 5 | 2.0 | 120s |
| **文件IO** | 2 | 1.5 | 30s |

### 10.2 兜底策略设计原则

1. **局部降级**：某个节点失败不影响整体流程
2. **数据完整性**：优先保证核心数据（Chunk）不丢失
3. **可追溯**：记录使用了兜底策略的任务

---

**版本**: 1.0  
**更新时间**: 2025年

