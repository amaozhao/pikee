# 日志使用指南

## 概述

项目使用 Python 内置的 `logging` 模块，并通过自定义工具进行配置和增强。

## 架构说明

```
┌─────────────────────────────────────────┐
│     应用启动 (main.py / server.py)      │
│   setup_logger() - 配置日志系统         │
└─────────────────┬───────────────────────┘
                  │
                  ├──> 设置日志级别
                  ├──> 配置输出格式
                  ├──> 添加控制台输出
                  └──> 添加文件输出（可选）
                  │
┌─────────────────▼───────────────────────┐
│          业务代码 (services/)            │
│   logging.getLogger(__name__) 获取 logger│
└─────────────────┬───────────────────────┘
                  │
                  ├──> logger.info()
                  ├──> logger.warning()
                  ├──> logger.error()
                  └──> logger.debug()
                  │
┌─────────────────▼───────────────────────┐
│            日志输出                      │
│   按照 setup_logger() 的配置输出         │
└─────────────────────────────────────────┘
```

## 使用方法

### 1. 应用启动时配置（一次性）

在应用入口配置日志系统：

```python
# pikee/main.py (HTTP API)
from pathlib import Path
from pikee.infrastructure.config.settings import get_settings
from pikee.infrastructure.utils.logger import setup_logger

settings = get_settings()

# 配置根日志记录器
logger = setup_logger(
    name="pikee",
    settings=settings,
    log_file=Path("logs/pikee.log") if not settings.debug else None,
)

logger.info("应用启动")
```

```python
# pikee/grpc/server.py (gRPC 服务)
from pikee.infrastructure.utils.logger import setup_logger

settings = get_settings()
logger = setup_logger("pikee.grpc", settings)

logger.info("gRPC 服务器启动")
```

### 2. 业务代码中使用

#### 方式一：模块级别定义（推荐）

```python
import logging

# 模块顶部定义
logger = logging.getLogger(__name__)

class MyService:
    def process(self, data: str) -> str:
        logger.info(f"处理数据: {data}")
        
        try:
            result = self._do_work(data)
            logger.debug(f"处理结果: {result}")
            return result
        except ValueError as e:
            logger.warning(f"数据验证失败: {e}")
            raise
        except Exception as e:
            logger.error(f"处理失败: {e}", exc_info=True)
            raise
```

#### 方式二：使用 LoggerMixin（类中使用）

```python
from pikee.infrastructure.utils.logger import LoggerMixin

class MyService(LoggerMixin):
    """继承 LoggerMixin 自动拥有 self.logger."""
    
    def process(self, data: str) -> str:
        # 直接使用 self.logger
        self.logger.info(f"处理数据: {data}")
        
        try:
            result = self._do_work(data)
            self.logger.debug(f"处理结果: {result}")
            return result
        except Exception as e:
            self.logger.error(f"处理失败: {e}", exc_info=True)
            raise
    
    def _do_work(self, data: str) -> str:
        self.logger.debug("执行内部逻辑")
        return data.upper()
```

#### 方式三：动态获取（函数中使用）

```python
from pikee.infrastructure.utils.logger import get_logger

def process_document(doc_id: str) -> None:
    """处理文档."""
    logger = get_logger(__name__)
    logger.info(f"处理文档: {doc_id}")
    
    # 业务逻辑...
```

## 日志级别使用建议

| 级别 | 使用场景 | 示例 |
|------|---------|------|
| **DEBUG** | 详细的调试信息，开发时使用 | `logger.debug(f"变量值: {value}")` |
| **INFO** | 关键流程信息，记录正常运行状态 | `logger.info("文档处理完成")` |
| **WARNING** | 警告信息，不影响运行但需注意 | `logger.warning("缓存未命中")` |
| **ERROR** | 错误信息，影响功能但不崩溃 | `logger.error("API 调用失败", exc_info=True)` |
| **CRITICAL** | 严重错误，系统无法继续运行 | `logger.critical("数据库连接失败")` |

## 日志记录最佳实践

### ✅ 推荐做法

```python
# 1. 记录关键业务流程
logger.info(f"开始处理文档: doc_id={doc.id}, size={doc.size}")

# 2. 记录错误时包含上下文和堆栈
try:
    result = api_call()
except Exception as e:
    logger.error(
        f"API 调用失败: url={url}, method={method}",
        exc_info=True,  # 包含完整堆栈信息
    )
    raise

# 3. 使用结构化日志（易于解析）
logger.info(f"处理完成: chunks={len(chunks)}, atoms={len(atoms)}, duration={elapsed:.2f}s")

# 4. 记录性能指标
from time import time
start = time()
result = heavy_operation()
logger.info(f"操作耗时: {time() - start:.3f}s")
```

### ❌ 不推荐做法

```python
# 1. 避免在循环中大量 INFO 日志
for item in large_list:
    logger.info(f"处理: {item}")  # ❌ 太多日志

# 改进：
logger.info(f"开始批量处理: total={len(large_list)}")
for i, item in enumerate(large_list):
    if i % 100 == 0:
        logger.debug(f"进度: {i}/{len(large_list)}")  # ✅ DEBUG 级别

# 2. 避免记录敏感信息
logger.info(f"用户登录: password={password}")  # ❌ 泄露密码
logger.info(f"API调用: key={api_key}")  # ❌ 泄露密钥

# 改进：
logger.info(f"用户登录: user={username}")  # ✅ 只记录用户名
logger.debug(f"API调用: key={api_key[:8]}***")  # ✅ 部分遮蔽

# 3. 避免字符串拼接
logger.info("处理文档 " + doc_id + " 完成")  # ❌ 字符串拼接

# 改进：
logger.info(f"处理文档 {doc_id} 完成")  # ✅ f-string
```

## 实际使用示例

### 示例 1：文档处理服务

```python
import logging
from typing import List

from pikee.pipeline.models.chunk import Chunk
from pikee.pipeline.models.document import Document

logger = logging.getLogger(__name__)

class DocumentProcessor:
    """文档处理器."""
    
    def process(self, document: Document) -> List[Chunk]:
        """处理文档.
        
        Args:
            document: 文档对象
            
        Returns:
            Chunk 列表
        """
        logger.info(
            f"开始处理文档: id={document.id}, "
            f"size={document.file_size}, type={document.file_type}"
        )
        
        try:
            # 1. 加载文档
            logger.debug("加载文档内容")
            content = self._load_document(document)
            
            # 2. 切分文档
            logger.info(f"切分文档: content_length={len(content)}")
            chunks = self._chunk_document(content)
            logger.info(f"切分完成: chunks={len(chunks)}")
            
            # 3. 提取特征
            logger.debug("提取文档特征")
            self._extract_features(chunks)
            
            logger.info(f"文档处理完成: id={document.id}, chunks={len(chunks)}")
            return chunks
            
        except FileNotFoundError as e:
            logger.error(f"文件不存在: path={document.file_path}", exc_info=True)
            raise
        except Exception as e:
            logger.error(
                f"文档处理失败: id={document.id}, error={type(e).__name__}",
                exc_info=True,
            )
            raise
```

### 示例 2：使用 LoggerMixin

```python
from pikee.infrastructure.utils.logger import LoggerMixin
from pikee.infrastructure.utils.metrics import get_metrics_collector

class VectorStoreBuilder(LoggerMixin):
    """向量存储构建器."""
    
    def build(self, chunks: List[Chunk]) -> bool:
        """构建向量存储.
        
        Args:
            chunks: Chunk 列表
            
        Returns:
            是否成功
        """
        self.logger.info(f"开始构建向量存储: chunks={len(chunks)}")
        
        metrics = get_metrics_collector()
        
        try:
            # 使用计时器和指标
            with metrics.timer("vector_build_duration"):
                for i, chunk in enumerate(chunks):
                    if i % 100 == 0:
                        self.logger.debug(f"进度: {i}/{len(chunks)}")
                    
                    self._process_chunk(chunk)
                    metrics.increment("chunks_processed")
            
            self.logger.info(f"向量存储构建完成: total={len(chunks)}")
            return True
            
        except Exception as e:
            self.logger.error(f"构建失败: {e}", exc_info=True)
            metrics.increment("build_failures")
            return False
```

### 示例 3：Prefect Task 中使用

```python
from prefect import task
import logging

logger = logging.getLogger(__name__)

@task(name="load_document", retries=3)
def load_document(file_path: str) -> Document:
    """加载文档 Task.
    
    Args:
        file_path: 文件路径
        
    Returns:
        文档对象
    """
    logger.info(f"[Task] 加载文档: {file_path}")
    
    try:
        document = Document.load(file_path)
        logger.info(f"[Task] 加载成功: size={document.file_size}")
        return document
    except Exception as e:
        logger.error(f"[Task] 加载失败: {e}", exc_info=True)
        raise

@task(name="chunk_document", retries=2)
def chunk_document(document: Document) -> List[Chunk]:
    """切分文档 Task."""
    logger.info(f"[Task] 切分文档: id={document.id}")
    
    chunker = DocumentChunker()
    chunks = chunker.chunk_document(document)
    
    logger.info(f"[Task] 切分完成: chunks={len(chunks)}")
    return chunks
```

## 配置说明

### 日志级别配置

在 Apollo 或 `.env` 中配置：

```bash
# .env
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

### 日志文件配置

```python
# 开发环境：只输出到控制台
logger = setup_logger("pikee", settings)

# 生产环境：同时输出到文件
logger = setup_logger(
    "pikee",
    settings,
    log_file=Path("logs/pikee.log"),
)
```

### 日志格式

默认格式（在 `setup_logger` 中定义）：

```
2025-11-14 15:30:45 - pikee.pipeline.chunker - INFO - 开始切分文档
└─────┬──────┘   └──────────┬──────────┘   └─┬─┘   └───────┬────────┘
    时间戳              模块名             级别        消息内容
```

## 故障排查

### 问题 1：看不到日志输出

**原因**：日志级别设置过高

**解决**：
```python
# 检查配置
settings = get_settings()
print(f"日志级别: {settings.log_level}")

# 临时降低级别
logger.setLevel(logging.DEBUG)
```

### 问题 2：日志重复输出

**原因**：多次调用 `setup_logger`

**解决**：确保在应用启动时只调用一次

```python
# ❌ 错误：每次都配置
def my_function():
    logger = setup_logger("pikee", settings)  # 重复配置
    logger.info("...")

# ✅ 正确：只配置一次
# main.py
setup_logger("pikee", settings)

# 业务代码
logger = logging.getLogger(__name__)
logger.info("...")
```

### 问题 3：日志文件不存在

**原因**：目录不存在

**解决**：`setup_logger` 会自动创建目录

```python
log_file = Path("logs/app.log")
# 会自动创建 logs/ 目录
logger = setup_logger("pikee", settings, log_file=log_file)
```

## 总结

1. **应用启动时**：使用 `setup_logger()` 配置日志系统（一次性）
2. **业务代码中**：使用 `logging.getLogger(__name__)` 获取 logger
3. **类中使用**：可以继承 `LoggerMixin` 获得 `self.logger`
4. **遵循最佳实践**：合理使用日志级别，记录关键信息，避免敏感数据

现有代码中的 `logger = logging.getLogger(__name__)` **无需修改**，它们会自动使用 `setup_logger()` 的配置！

