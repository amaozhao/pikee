# PIKE-RAG æ–‡æ¡£å¤„ç†ä¸æ™ºèƒ½åˆ‡åˆ†è¯¦è§£

## ğŸ¯ å®é™…åº”ç”¨ä¸­çš„æ ¸å¿ƒæŒ‘æˆ˜

åœ¨å®é™…éƒ¨ç½² RAG ç³»ç»Ÿæ—¶ï¼Œæ‚¨ä¼šé¢ä¸´ä»¥ä¸‹å…³é”®é—®é¢˜ï¼š

### é—®é¢˜ 1: å¤šç§æ–‡æ¡£æ ¼å¼
```
å®é™…åœºæ™¯ï¼š
âœ… éœ€è¦å¤„ç†çš„æ ¼å¼ï¼štxt, md, pdf, doc, docx, ppt, pptx, excel, csv, html, å›¾ç‰‡...
âŒ ç®€å•çš„æ–‡æœ¬å¯¼å…¥ï¼šåªèƒ½å¤„ç†çº¯æ–‡æœ¬
```

### é—®é¢˜ 2: åˆé€‚çš„ Chunk å¤§å°
```
å›°å¢ƒï¼š
- Chunk å¤ªå° â†’ ä¸Šä¸‹æ–‡ä¸å®Œæ•´ï¼Œä¿¡æ¯ç¢ç‰‡åŒ–
- Chunk å¤ªå¤§ â†’ æ£€ç´¢ä¸ç²¾ç¡®ï¼Œå™ªéŸ³å¤šï¼ŒToken æ¶ˆè€—å¤§
- ä¸åŒæ–‡æ¡£éœ€è¦ä¸åŒçš„åˆ‡åˆ†ç­–ç•¥
```

### é—®é¢˜ 3: è¯­ä¹‰è¾¹ç•Œ
```
ä¼ ç»Ÿå›ºå®šé•¿åº¦åˆ‡åˆ†ï¼š
"... å¥‰ä¿Šæ˜Šæ˜¯éŸ©å›½è‘—åå¯¼æ¼”ã€‚ä»–çš„ä»£è¡¨ä½œå“åŒ…æ‹¬ã€Šæ€äººå›å¿†ã€‹å’Œã€Š" [åˆ‡æ–­]
"å¯„ç”Ÿè™«ã€‹ã€‚ã€Šå¯„ç”Ÿè™«ã€‹åœ¨2020å¹´å¥¥æ–¯å¡è·å¾—æœ€ä½³å½±ç‰‡å¥–..."

é—®é¢˜ï¼šåœ¨å¥å­ä¸­é—´åˆ‡æ–­ï¼Œç ´åè¯­ä¹‰å®Œæ•´æ€§
```

PIKE-RAG æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼è®©æˆ‘ä»¬é€ä¸€æ·±å…¥ã€‚

---

## ğŸ“ Part 1: å¤šæ ¼å¼æ–‡æ¡£åŠ è½½

### 1.1 æ”¯æŒçš„æ–‡æ¡£æ ¼å¼

PIKE-RAG åŸºäº `LangChain` çš„æ–‡æ¡£åŠ è½½å™¨ï¼Œæ”¯æŒä¸°å¯Œçš„æ ¼å¼ï¼š

```python
# pikerag/document_loaders/common.py

class DocumentType(Enum):
    csv = ["csv"]
    excel = ["xlsx", "xls"]
    markdown = ["md"]
    pdf = ["pdf"]
    text = ["txt"]
    word = ["docx", "doc"]
```

**å½“å‰å·²å®ç°çš„æ ¼å¼ï¼š**

| æ ¼å¼ç±»åˆ« | æ–‡ä»¶æ‰©å±•å | åŠ è½½å™¨ | è¯´æ˜ |
|---------|-----------|--------|------|
| æ–‡æœ¬ | `.txt` | `TextLoader` | çº¯æ–‡æœ¬æ–‡ä»¶ |
| Markdown | `.md` | `UnstructuredMarkdownLoader` | ä¿ç•™æ ¼å¼ç»“æ„ |
| PDF | `.pdf` | `UnstructuredPDFLoader` | æ”¯æŒå›¾ç‰‡å’Œè¡¨æ ¼æå– |
| Word | `.doc`, `.docx` | `UnstructuredWordDocumentLoader` | æ–‡æ¡£ç»“æ„ä¿ç•™ |
| Excel | `.xlsx`, `.xls` | `UnstructuredExcelLoader` | è¡¨æ ¼æ•°æ® |
| CSV | `.csv` | `CSVLoader` | ç»“æ„åŒ–æ•°æ® |

---

### 1.2 è‡ªåŠ¨æ–‡æ¡£ç±»å‹è¯†åˆ«

```python
# pikerag/document_loaders/utils.py

def infer_file_type(file_path: str) -> Optional[DocumentType]:
    """æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç±»å‹"""
    if os.path.exists(file_path):
        file_extension = Path(file_path).suffix[1:]  # å»æ‰ç‚¹å·
        for e in DocumentType:
            if file_extension in e.value:
                return e
        
        print(f"æ–‡ä»¶ç±»å‹æ— æ³•è¯†åˆ«: {file_path}.")
        return None
    
    return None
```

**ä½¿ç”¨ç¤ºä¾‹ï¼š**
```python
file_path = "data/documents/report.pdf"
doc_type = infer_file_type(file_path)  # è‡ªåŠ¨è¯†åˆ«ä¸º DocumentType.pdf
```

---

### 1.3 ç»Ÿä¸€çš„æ–‡æ¡£åŠ è½½æ¥å£

```python
# pikerag/document_loaders/utils.py

def get_loader(file_path: str, file_type: DocumentType = None) -> Optional[BaseLoader]:
    """è·å–é€‚åˆçš„æ–‡æ¡£åŠ è½½å™¨
    
    Args:
        file_path: æ–‡æ¡£è·¯å¾„
        file_type: æ–‡æ¡£ç±»å‹ï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨è¯†åˆ«ï¼‰
    
    Returns:
        BaseLoader: å¯¹åº”çš„æ–‡æ¡£åŠ è½½å™¨
    """
    # 1. è‡ªåŠ¨è¯†åˆ«æ–‡æ¡£ç±»å‹ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    inferred_file_type = file_type
    if file_type is None:
        inferred_file_type = infer_file_type(file_path)
        if inferred_file_type is None:
            return None
    
    # 2. æ ¹æ®æ–‡æ¡£ç±»å‹è¿”å›å¯¹åº”çš„åŠ è½½å™¨
    if inferred_file_type == DocumentType.csv:
        from langchain_community.document_loaders import CSVLoader
        return CSVLoader(file_path, encoding="utf-8", autodetect_encoding=True)
    
    elif inferred_file_type == DocumentType.excel:
        from langchain_community.document_loaders import UnstructuredExcelLoader
        return UnstructuredExcelLoader(file_path)
    
    elif inferred_file_type == DocumentType.markdown:
        from langchain_community.document_loaders.markdown import UnstructuredMarkdownLoader
        return UnstructuredMarkdownLoader(file_path)
    
    elif inferred_file_type == DocumentType.text:
        from langchain_community.document_loaders import TextLoader
        return TextLoader(file_path, encoding="utf-8", autodetect_encoding=True)
    
    elif inferred_file_type == DocumentType.word:
        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
        return UnstructuredWordDocumentLoader(file_path)
    
    elif inferred_file_type == DocumentType.pdf:
        from langchain_community.document_loaders import UnstructuredPDFLoader
        return UnstructuredPDFLoader(file_path)
    
    else:
        print(f"æœªå®šä¹‰è¯¥ç±»å‹çš„æ–‡æ¡£åŠ è½½å™¨: {inferred_file_type}")
        return None
```

---

### 1.4 å®é™…ä½¿ç”¨ç¤ºä¾‹

#### ç¤ºä¾‹ 1: åŠ è½½å•ä¸ªæ–‡æ¡£

```python
from pikerag.document_loaders import get_loader

# è‡ªåŠ¨è¯†åˆ«å¹¶åŠ è½½ PDF
loader = get_loader("data/documents/report.pdf")
documents = loader.load()

# documents æ˜¯ Document å¯¹è±¡åˆ—è¡¨
for doc in documents:
    print(doc.page_content)  # æ–‡æ¡£å†…å®¹
    print(doc.metadata)      # å…ƒæ•°æ®ï¼ˆæ–‡ä»¶åã€é¡µç ç­‰ï¼‰
```

#### ç¤ºä¾‹ 2: æ‰¹é‡åŠ è½½å¤šç§æ ¼å¼

```python
from pikerag.document_loaders import get_loader
from pikerag.utils.walker import list_files_recursively

# é€’å½’è·å–ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£
file_infos = list_files_recursively(
    directory="data/documents/",
    extensions=[".txt", ".pdf", ".docx", ".md"]  # æ”¯æŒå¤šç§æ ¼å¼
)

all_documents = []
for doc_name, doc_path in file_infos:
    loader = get_loader(doc_path)
    if loader is not None:
        docs = loader.load()
        # æ·»åŠ å…ƒæ•°æ®
        for doc in docs:
            doc.metadata["filename"] = doc_name
        all_documents.extend(docs)

print(f"æˆåŠŸåŠ è½½ {len(all_documents)} ä¸ªæ–‡æ¡£")
```

---

### 1.5 ç‰¹æ®Šæ ¼å¼å¤„ç†

#### PDF æ–‡æ¡£ï¼šå›¾ç‰‡å’Œè¡¨æ ¼æå–

PIKE-RAG ä½¿ç”¨ `UnstructuredPDFLoader`ï¼Œå®ƒå¯ä»¥ï¼š
- âœ… æå–æ–‡æœ¬å†…å®¹
- âœ… è¯†åˆ«è¡¨æ ¼ç»“æ„
- âœ… OCR æå–å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼ˆéœ€è¦å®‰è£… tesseractï¼‰

**ä¾èµ–å®‰è£…ï¼š**
```bash
# å®‰è£… PDF å¤„ç†ç›¸å…³ä¾èµ–
pip install unstructured pdf2image pikepdf
pip install unstructured_inference  # ç”¨äºè¡¨æ ¼å’Œå›¾ç‰‡æ£€æµ‹
pip install pytesseract unstructured_pytesseract  # OCR æ”¯æŒ

# Linux ä¸Šå®‰è£… tesseract
apt-get install tesseract-ocr

# macOS ä¸Šå®‰è£…
brew install tesseract

# Windows ä¸Šéœ€è¦å•ç‹¬ä¸‹è½½å®‰è£… tesseract
```

**é…ç½® OCRï¼š**
```python
from langchain_community.document_loaders import UnstructuredPDFLoader

loader = UnstructuredPDFLoader(
    "document.pdf",
    mode="elements",  # ä¿ç•™æ–‡æ¡£ç»“æ„ä¿¡æ¯
    strategy="hi_res"  # é«˜åˆ†è¾¨ç‡å¤„ç†ï¼Œå¯ç”¨å›¾ç‰‡å’Œè¡¨æ ¼æ£€æµ‹
)
documents = loader.load()
```

#### Word æ–‡æ¡£ï¼šä¿ç•™æ ¼å¼

```python
from langchain_community.document_loaders import UnstructuredWordDocumentLoader

loader = UnstructuredWordDocumentLoader("document.docx")
documents = loader.load()

# å…ƒæ•°æ®åŒ…å«ï¼š
# - æ®µè½æ ·å¼
# - æ ‡é¢˜å±‚çº§
# - è¡¨æ ¼ä¿¡æ¯
```

#### Excel è¡¨æ ¼ï¼šç»“æ„åŒ–æ•°æ®

```python
from langchain_community.document_loaders import UnstructuredExcelLoader

loader = UnstructuredExcelLoader("data.xlsx")
documents = loader.load()

# æ¯ä¸ª sheet ä¼šè¢«è½¬æ¢ä¸ºä¸€ä¸ªæˆ–å¤šä¸ª Document
# ä¿ç•™è¡¨æ ¼çš„è¡Œåˆ—ç»“æ„
```

---

### 1.6 æ‰©å±•ï¼šæ·»åŠ æ–°çš„æ–‡æ¡£ç±»å‹

å¦‚æœéœ€è¦æ”¯æŒå…¶ä»–æ ¼å¼ï¼ˆå¦‚ PPTã€HTMLã€å›¾ç‰‡ç­‰ï¼‰ï¼Œå¯ä»¥è½»æ¾æ‰©å±•ï¼š

```python
# Step 1: åœ¨ DocumentType ä¸­æ·»åŠ æ–°ç±»å‹
class DocumentType(Enum):
    csv = ["csv"]
    excel = ["xlsx", "xls"]
    markdown = ["md"]
    pdf = ["pdf"]
    text = ["txt"]
    word = ["docx", "doc"]
    # æ–°å¢ç±»å‹
    powerpoint = ["ppt", "pptx"]
    html = ["html", "htm"]
    image = ["jpg", "jpeg", "png"]

# Step 2: åœ¨ get_loader ä¸­æ·»åŠ å¯¹åº”çš„åŠ è½½å™¨
def get_loader(file_path: str, file_type: DocumentType = None) -> Optional[BaseLoader]:
    # ... å·²æœ‰ä»£ç  ...
    
    elif inferred_file_type == DocumentType.powerpoint:
        from langchain_community.document_loaders import UnstructuredPowerPointLoader
        return UnstructuredPowerPointLoader(file_path)
    
    elif inferred_file_type == DocumentType.html:
        from langchain_community.document_loaders import UnstructuredHTMLLoader
        return UnstructuredHTMLLoader(file_path)
    
    elif inferred_file_type == DocumentType.image:
        from langchain_community.document_loaders import UnstructuredImageLoader
        return UnstructuredImageLoader(file_path)
    
    # ... å…¶ä»–ä»£ç  ...
```

---

## âœ‚ï¸ Part 2: æ™ºèƒ½åˆ‡åˆ†ç­–ç•¥

### 2.1 ä¸ºä»€ä¹ˆéœ€è¦æ™ºèƒ½åˆ‡åˆ†ï¼Ÿ

**ä¼ ç»Ÿå›ºå®šé•¿åº¦åˆ‡åˆ†çš„é—®é¢˜ï¼š**

```python
# ä¼ ç»Ÿæ–¹æ³•ï¼šæŒ‰å­—ç¬¦æ•°åˆ‡åˆ†
text = "é•¿ç¯‡æ–‡æ¡£å†…å®¹..."
chunk_size = 500

chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
```

**é—®é¢˜ï¼š**
1. âŒ åœ¨å¥å­ä¸­é—´åˆ‡æ–­
2. âŒ ç ´åè¯­ä¹‰å®Œæ•´æ€§
3. âŒ æ®µè½å’Œç« èŠ‚ç»“æ„ä¸¢å¤±
4. âŒ æ£€ç´¢æ•ˆæœå·®

**ç¤ºä¾‹ï¼š**
```
Chunk 1: "... å¥‰ä¿Šæ˜Šæ˜¯éŸ©å›½è‘—åå¯¼æ¼”ã€‚ä»–çš„ä»£è¡¨ä½œå“åŒ…æ‹¬ã€Šæ€äººå›å¿†ã€‹å’Œã€Š"
Chunk 2: "å¯„ç”Ÿè™«ã€‹ã€‚ã€Šå¯„ç”Ÿè™«ã€‹åœ¨2020å¹´å¥¥æ–¯å¡è·å¾—æœ€ä½³å½±ç‰‡å¥–..."
```
â†’ ç¬¬ä¸€ä¸ª chunk ä¸­çš„ã€Šå¯„ç”Ÿè™«ã€‹ä¸å®Œæ•´ï¼

---

### 2.2 PIKE-RAG çš„ä¸¤ç§åˆ‡åˆ†ç­–ç•¥

PIKE-RAG æä¾›ä¸¤ç§åˆ‡åˆ†å™¨ï¼Œé€‚ç”¨äºä¸åŒåœºæ™¯ï¼š

#### ç­–ç•¥ 1: RecursiveSentenceSplitter - åŸºäºå¥å­çš„é€’å½’åˆ‡åˆ†

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¿«é€Ÿå¤„ç†å¤§é‡æ–‡æ¡£
- âœ… ä¸éœ€è¦ LLM æ”¯æŒ
- âœ… å¯¹åˆ‡åˆ†è´¨é‡è¦æ±‚ä¸æ˜¯æé«˜

**æ ¸å¿ƒæ€æƒ³ï¼š**
```
1. ä½¿ç”¨ Spacy è¿›è¡Œå¥å­åˆ†å‰²
2. æŒ‰å¥å­æ•°é‡åˆ‡åˆ†ï¼ˆè€Œéå­—ç¬¦æ•°ï¼‰
3. ä¿è¯åœ¨å¥å­è¾¹ç•Œå¤„åˆ‡åˆ†
4. æ”¯æŒé‡å ä»¥ä¿ç•™ä¸Šä¸‹æ–‡
```

**ä»£ç å®ç°ï¼š**

```python
# pikerag/document_transformers/splitter/recursive_sentence_splitter.py

class RecursiveSentenceSplitter(TextSplitter):
    def __init__(
        self,
        lang: str = "en",           # è¯­è¨€ï¼šenï¼ˆè‹±æ–‡ï¼‰æˆ– zhï¼ˆä¸­æ–‡ï¼‰
        chunk_size: int = 12,       # æ¯ä¸ª chunk åŒ…å«çš„å¥å­æ•°
        chunk_overlap: int = 4,     # é‡å çš„å¥å­æ•°
        **kwargs,
    ):
        super().__init__(chunk_size, chunk_overlap)
        self._stride = self._chunk_size - self._chunk_overlap
        
        # åŠ è½½ Spacy æ¨¡å‹
        self._load_model(lang)
    
    def split_text(self, text: str) -> List[str]:
        # 1. ä½¿ç”¨ Spacy åˆ†å¥
        doc = self._nlp(text)
        sents = [sent.text.strip() for sent in doc.sents]
        
        # 2. æŒ‰å¥å­æ•°é‡åˆ‡åˆ†
        segments = []
        for i in range(0, len(sents), self._stride):
            segment = " ".join(sents[i : i + self._chunk_size])
            segments.append(segment)
            if i + self._chunk_size >= len(sents):
                break
        
        return segments
```

**é…ç½®ç¤ºä¾‹ï¼š**

```yaml
# è‹±æ–‡æ–‡æ¡£
splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: en
    chunk_size: 12    # æ¯ä¸ª chunk 12 ä¸ªå¥å­
    chunk_overlap: 4  # é‡å  4 ä¸ªå¥å­
```

```yaml
# ä¸­æ–‡æ–‡æ¡£
splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: zh
    chunk_size: 10
    chunk_overlap: 3
```

**æ•ˆæœç¤ºä¾‹ï¼š**

```
åŸæ–‡ï¼š
Sentence 1. Sentence 2. Sentence 3. Sentence 4. Sentence 5. 
Sentence 6. Sentence 7. Sentence 8. Sentence 9. Sentence 10.

chunk_size=4, chunk_overlap=1:

Chunk 1: Sentence 1. Sentence 2. Sentence 3. Sentence 4.
Chunk 2:                         Sentence 4. Sentence 5. Sentence 6. Sentence 7.
Chunk 3:                                                 Sentence 7. Sentence 8. Sentence 9. Sentence 10.
```

**ä¼˜ç‚¹ï¼š**
- âœ… å¿«é€Ÿï¼ˆæ— éœ€ LLMï¼‰
- âœ… åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼Œä¸ç ´åå¥å­å®Œæ•´æ€§
- âœ… æ”¯æŒä¸­è‹±æ–‡

**å±€é™ï¼š**
- âš ï¸ æ— æ³•ç†è§£è¯­ä¹‰å…³è”
- âš ï¸ å¯èƒ½åœ¨æ®µè½ä¸­é—´åˆ‡åˆ†
- âš ï¸ æ— æ³•è¯†åˆ«ä¸»é¢˜è¾¹ç•Œ

---

#### ç­–ç•¥ 2: LLMPoweredRecursiveSplitter - LLM é©±åŠ¨çš„æ™ºèƒ½åˆ‡åˆ† â­

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… éœ€è¦é«˜è´¨é‡åˆ‡åˆ†
- âœ… æ–‡æ¡£å†…å®¹å¤æ‚ï¼Œä¸»é¢˜å¤šæ ·
- âœ… æœ‰ LLM API æ”¯æŒ

**æ ¸å¿ƒåˆ›æ–°ï¼š**
```
1. åˆå§‹åˆ‡åˆ†ï¼šå…ˆç”¨ä¼ ç»Ÿæ–¹æ³•ç²—åˆ‡
2. LLM è¯„ä¼°ï¼šè®© LLM ç†è§£å†…å®¹å’Œè¯­ä¹‰è¾¹ç•Œ
3. æ™ºèƒ½è°ƒæ•´ï¼šåœ¨è¯­ä¹‰è¾¹ç•Œå¤„é‡æ–°åˆ‡åˆ†
4. ç”Ÿæˆæ‘˜è¦ï¼šä¸ºæ¯ä¸ª chunk ç”Ÿæˆæ‘˜è¦
5. è¿­ä»£ä¼˜åŒ–ï¼šé€’å½’å¤„ç†ç›´åˆ°æ»¡è¶³è¦æ±‚
```

**å·¥ä½œæµç¨‹ï¼š**

```
åŸå§‹æ–‡æ¡£
    â†“
ã€æ­¥éª¤ 1ã€‘ä½¿ç”¨ RecursiveCharacterTextSplitter åˆæ­¥åˆ‡åˆ†
    â†“
[Chunk1, Chunk2, Chunk3, ...]
    â†“
ã€æ­¥éª¤ 2ã€‘å¯¹ç¬¬ä¸€ä¸ª Chunk ç”Ÿæˆæ‘˜è¦
    â†“ (LLM è°ƒç”¨ 1)
"è¿™éƒ¨åˆ†ä¸»è¦è®²è¿°äº†å¥‰ä¿Šæ˜Šçš„æ—©æœŸä½œå“"
    â†“
ã€æ­¥éª¤ 3ã€‘LLM åˆ¤æ–­ï¼šå‰ä¸¤ä¸ª Chunk æ˜¯å¦åº”è¯¥åœ¨ä¸åŒè¾¹ç•Œåˆ‡åˆ†ï¼Ÿ
    â†“ (LLM è°ƒç”¨ 2)
å†³ç­–: "åº”è¯¥åœ¨ç¬¬ 15 è¡Œåˆ‡åˆ†ï¼Œå‰é¢è®²æ—©æœŸä½œå“ï¼Œåé¢è®²ã€Šå¯„ç”Ÿè™«ã€‹"
    â†“
ã€æ­¥éª¤ 4ã€‘æ ¹æ® LLM å»ºè®®é‡æ–°åˆ‡åˆ†
    â†“
[æ–°Chunk1, æ–°Chunk2, ...]
    â†“
ã€æ­¥éª¤ 5ã€‘ä¸ºæœ€åä¸€ä¸ª Chunk ä¼˜åŒ–æ‘˜è¦
    â†“ (LLM è°ƒç”¨ 3)
"ç»“åˆå‰æ–‡ï¼Œè¿™éƒ¨åˆ†ä¸»è¦è®²è¿°ã€Šå¯„ç”Ÿè™«ã€‹çš„è·å¥–æƒ…å†µ"
    â†“
ã€æ­¥éª¤ 6ã€‘é€’å½’å¤„ç†å‰©ä½™æ–‡æ¡£
    â†“
æœ€ç»ˆé«˜è´¨é‡ Chunks
```

**ä»£ç å®ç°ï¼š**

```python
# pikerag/document_transformers/splitter/llm_powered_recursive_splitter.py

class LLMPoweredRecursiveSplitter(TextSplitter):
    def __init__(
        self,
        llm_client: BaseLLMClient,                      # LLM å®¢æˆ·ç«¯
        first_chunk_summary_protocol: CommunicationProtocol,  # é¦–æ¬¡æ‘˜è¦æç¤º
        last_chunk_summary_protocol: CommunicationProtocol,   # æœ€åæ‘˜è¦æç¤º
        chunk_resplit_protocol: CommunicationProtocol,        # é‡åˆ‡åˆ†æç¤º
        llm_config: dict = {},
        chunk_size: int = 4000,     # åˆå§‹ chunk å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        chunk_overlap: int = 200,   # é‡å å­—ç¬¦æ•°
        **kwargs,
    ):
        super().__init__(chunk_size, chunk_overlap, ...)
        
        # åŸºç¡€åˆ‡åˆ†å™¨ï¼ˆç”¨äºåˆæ­¥åˆ‡åˆ†ï¼‰
        self._base_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs,
        )
        
        self._llm_client = llm_client
        self._llm_config = llm_config
        
        # ä¸‰ä¸ª LLM é€šä¿¡åè®®
        self._first_chunk_summary_protocol = first_chunk_summary_protocol
        self._last_chunk_summary_protocol = last_chunk_summary_protocol
        self._chunk_resplit_protocol = chunk_resplit_protocol
```

**æ ¸å¿ƒæ–¹æ³•ï¼š**

```python
def split_documents(self, documents: List[Document]) -> List[Document]:
    ret_docs = []
    
    for doc in documents:
        text = doc.page_content
        metadata = doc.metadata
        
        # 1. ç”Ÿæˆç¬¬ä¸€ä¸ª chunk çš„æ‘˜è¦
        chunk_summary = self._get_first_chunk_summary(text, **metadata)
        
        # 2. åˆæ­¥åˆ‡åˆ†
        chunks = self._base_splitter.split_text(text)
        
        # 3. è¿­ä»£å¤„ç†
        while True:
            if len(chunks) == 1:
                # æœ€åä¸€ä¸ª chunkï¼šä¼˜åŒ–æ‘˜è¦
                chunk_summary = self._get_last_chunk_summary(
                    chunks[0], chunk_summary, **metadata
                )
                ret_docs.append(
                    Document(
                        page_content=chunks[0],
                        metadata={**metadata, "summary": chunk_summary}
                    )
                )
                break
            
            else:
                # è®© LLM é‡æ–°è¯„ä¼°åˆ‡åˆ†ç‚¹
                new_chunk, new_summary, next_summary, dropped_len = \
                    self._resplit_chunk_and_generate_summary(
                        text, chunks, chunk_summary, **metadata
                    )
                
                # æ·»åŠ æ–°åˆ‡åˆ†çš„ chunk
                ret_docs.append(
                    Document(
                        page_content=new_chunk,
                        metadata={**metadata, "summary": new_summary}
                    )
                )
                
                # æ›´æ–°å‰©ä½™æ–‡æœ¬
                text = text[dropped_len:].strip()
                chunk_summary = next_summary
                chunks = self._base_splitter.split_text(text)
    
    return ret_docs
```

---

### 2.3 LLM åˆ‡åˆ†çš„æç¤ºæ¨¡æ¿è¯¦è§£

#### æç¤º 1: é¦–æ¬¡æ‘˜è¦ç”Ÿæˆ

```python
# pikerag/prompts/chunking/recursive_splitter.py

chunk_summary_template = MessageTemplate(
    template=[
        ("system", "You are a helpful AI assistant good at document summarization."),
        ("user", """
# Source of the original text

The original text comes from {filename}ã€‚

# Original text

"partial original text":
{content}

# Task

Your task is to summarize the above "partial original text"

# Output

The output should contain the summary, do not add any redundant information.
"""),
    ],
    input_variables=["filename", "content"],
)
```

**LLM è¾“å‡ºç¤ºä¾‹ï¼š**
```
This part mainly introduces Bong Joon-ho's early works, including Memories of Murder 
and The Host, which established his reputation in Korean cinema.
```

---

#### æç¤º 2: æ™ºèƒ½é‡åˆ‡åˆ†

```python
chunk_resplit_template = MessageTemplate(
    template=[
        ("system", "You are a helpful AI assistant good at document chunking."),
        ("user", """
# Source of the original text

The original text comes from {filename}ã€‚

# Original text

generalization of "the first part" of "partial original text":
{summary}

"partial original text":
{content}

# Task

Your task:
1. Understand the generalization of "the first part" and the "partial original text";
2. Analyse the structure of "partial original text", Split it into "the first part" 
   and "the second part", no content can be missing.
3. Provide the "end line number" of "the first part".
4. Summarize "the first part"ã€‚
5. Summarize "the second part" considering the context.

# Output

The output should strictly follow the format below:

Thinking: Analyze the structure and think about how to split reasonably.

<result>
<chunk>
  <endline>end line number of "the first part"</endline>
  <summary>Summary of the "first part"</summary>
</chunk>
<chunk>
  <summary>Summary of the "second part"</summary>
</chunk>
</result>
"""),
    ],
    input_variables=["filename", "summary", "content", "max_line_number"],
)
```

**LLM è¾“å‡ºç¤ºä¾‹ï¼š**
```
Thinking: The text can be divided at line 15. Lines 0-15 discuss Bong Joon-ho's 
early career, while lines 16+ focus on Parasite and its Oscar success. This creates 
a natural thematic boundary.

<result>
<chunk>
  <endline>15</endline>
  <summary>This part covers Bong Joon-ho's early filmmaking career from 2000 to 2013</summary>
</chunk>
<chunk>
  <summary>This part describes the creation and success of Parasite at the 2020 Oscars</summary>
</chunk>
</result>
```

---

#### æç¤º 3: æœ€å Chunk æ‘˜è¦ä¼˜åŒ–

```python
chunk_summary_refinement_template = MessageTemplate(
    template=[
        ("system", "You are a helpful AI assistant good at summary refinement."),
        ("user", """
# Source of the original text

The original text comes from {filename}ã€‚

# Original text

generalization of "partial original text":
{summary}

"partial original text":
{content}

# Task

Your task is to summarize the above "partial original text" considering the 
generalization provided.

# Output

The output should contain the summary, do not add any redundant information.
"""),
    ],
    input_variables=["filename", "summary", "content"],
)
```

**LLM è¾“å‡ºç¤ºä¾‹ï¼š**
```
Building on the previous discussion of Parasite's success, this final part details 
Bong Joon-ho's acceptance speeches, the film's impact on Korean cinema, and its 
lasting legacy in breaking barriers for non-English films at major awards.
```

---

### 2.4 é…ç½®ç¤ºä¾‹ï¼šå¦‚ä½•é€‰æ‹©å’Œè°ƒä¼˜

#### é…ç½® 1: å¿«é€Ÿå¤„ç†ï¼ˆåŸºäºå¥å­ï¼‰

```yaml
# examples/configs/chunking_fast.yml

splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: en
    chunk_size: 12        # æ¯ä¸ª chunk 12 ä¸ªå¥å­
    chunk_overlap: 4      # é‡å  4 ä¸ªå¥å­
```

**é€‚ç”¨åœºæ™¯ï¼š**
- å¤§é‡æ–‡æ¡£ï¼ˆ>1000ä¸ªæ–‡ä»¶ï¼‰
- æ–‡æ¡£ç»“æ„ç®€å•
- é¢„ç®—æœ‰é™ï¼ˆæ—  LLM æˆæœ¬ï¼‰

---

#### é…ç½® 2: é«˜è´¨é‡åˆ‡åˆ†ï¼ˆLLM é©±åŠ¨ï¼‰

```yaml
# examples/biology/configs/chunking.yml

llm_client:
  module_path: pikerag.llm_client
  class_name: AzureOpenAIClient
  llm_config:
    model: gpt-4
    temperature: 0

chunking_protocol:
  module_path: pikerag.prompts.chunking
  chunk_summary: chunk_summary_protocol
  chunk_summary_refinement: chunk_summary_refinement_protocol
  chunk_resplit: chunk_resplit_protocol

splitter:
  module_path: pikerag.document_transformers
  class_name: LLMPoweredRecursiveSplitter
  args:
    separators:
      - "\n\n"      # é¦–å…ˆåœ¨æ®µè½è¾¹ç•Œåˆ‡åˆ†
      - "\n"        # ç„¶ååœ¨æ¢è¡Œå¤„åˆ‡åˆ†
      - "ã€‚"        # ä¸­æ–‡å¥å·
      - ". "        # è‹±æ–‡å¥å·
    is_separator_regex: False
    chunk_size: 512      # åˆå§‹ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦ï¼‰
    chunk_overlap: 0     # é€šè¿‡æ‘˜è¦è¿æ¥ï¼Œä¸éœ€è¦ç¡¬é‡å 
```

**é€‚ç”¨åœºæ™¯ï¼š**
- é«˜ä»·å€¼æ–‡æ¡£ï¼ˆæ³•å¾‹ã€åŒ»ç–—ã€ç ”ç©¶æŠ¥å‘Šï¼‰
- å¤æ‚æ–‡æ¡£ç»“æ„
- éœ€è¦é«˜è´¨é‡æ£€ç´¢

---

### 2.5 å¦‚ä½•ç¡®å®šåˆé€‚çš„ Chunk å¤§å°ï¼Ÿ

è¿™æ˜¯ä¸€ä¸ª**å¹³è¡¡é—®é¢˜**ï¼Œéœ€è¦è€ƒè™‘å¤šä¸ªå› ç´ ï¼š

#### å› ç´  1: Token é¢„ç®—

```python
# LLM ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
GPT-4: 8k / 32k / 128k tokens
GPT-3.5: 4k / 16k tokens

# è®¡ç®—å…¬å¼
max_chunks = (context_window - system_prompt - question - response_reserve) / chunk_size

# ç¤ºä¾‹ï¼šGPT-4 (8k tokens)
# - System prompt: ~200 tokens
# - Question: ~50 tokens
# - Response reserve: ~500 tokens
# - Available: 8192 - 200 - 50 - 500 = 7442 tokens
# - If chunk_size = 500 tokens: max_chunks = 7442 / 500 â‰ˆ 14 chunks
```

**å»ºè®®ï¼š**
```yaml
# å¯¹äº GPT-3.5 (4k)
chunk_size: 300-400 å­—ç¬¦

# å¯¹äº GPT-4 (8k)
chunk_size: 500-600 å­—ç¬¦

# å¯¹äº GPT-4 (32k)
chunk_size: 800-1000 å­—ç¬¦
```

---

#### å› ç´  2: æ–‡æ¡£ç±»å‹

```
çŸ­æ–‡æ¡£ï¼ˆæ–°é—»ã€åšå®¢ï¼‰:
    chunk_size: 300-500 å­—ç¬¦
    chunk_overlap: 50-100 å­—ç¬¦
    æ¯ä¸ª chunk 1-3 æ®µ

ä¸­ç­‰æ–‡æ¡£ï¼ˆæŠ¥å‘Šã€è®ºæ–‡ï¼‰:
    chunk_size: 500-800 å­—ç¬¦
    chunk_overlap: 100-150 å­—ç¬¦
    æ¯ä¸ª chunk 3-5 æ®µ

é•¿æ–‡æ¡£ï¼ˆä¹¦ç±ã€æ‰‹å†Œï¼‰:
    chunk_size: 800-1200 å­—ç¬¦
    chunk_overlap: 150-200 å­—ç¬¦
    æ¯ä¸ª chunk 5-8 æ®µ
```

---

#### å› ç´  3: æ£€ç´¢ç²¾åº¦è¦æ±‚

```
é«˜ç²¾åº¦æ£€ç´¢ï¼ˆéœ€è¦å‡†ç¡®åŒ¹é…ï¼‰:
    â†’ è¾ƒå°çš„ chunkï¼ˆ300-500 å­—ç¬¦ï¼‰
    â†’ æ›´ç²¾ç¡®çš„å‘é‡åŒ¹é…
    â†’ ä½†å¯èƒ½ä¸Šä¸‹æ–‡ä¸è¶³

å¹³è¡¡æ¨¡å¼:
    â†’ ä¸­ç­‰ chunkï¼ˆ500-800 å­—ç¬¦ï¼‰
    â†’ æ¨èè®¾ç½®

é«˜å¬å›ç‡ï¼ˆéœ€è¦å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰:
    â†’ è¾ƒå¤§çš„ chunkï¼ˆ800-1200 å­—ç¬¦ï¼‰
    â†’ æ›´ä¸°å¯Œçš„ä¸Šä¸‹æ–‡
    â†’ ä½†å¯èƒ½æœ‰å™ªéŸ³
```

---

#### å› ç´  4: è¯­è¨€ç‰¹ç‚¹

```
è‹±æ–‡:
    - å¹³å‡å•è¯é•¿åº¦: 5 å­—ç¬¦
    - å¹³å‡å¥å­: 15-20 å•è¯
    - å»ºè®® chunk_size: 500-800 å­—ç¬¦

ä¸­æ–‡:
    - å¹³å‡å­—ç¬¦å¯†åº¦æ›´é«˜
    - å¹³å‡å¥å­: 15-25 å­—ç¬¦
    - å»ºè®® chunk_size: 300-600 å­—ç¬¦
```

---

### 2.6 åŠ¨æ€ Chunk å¤§å°ç­–ç•¥ï¼ˆæ¨èï¼‰â­

**æ ¸å¿ƒæ€æƒ³ï¼š** ä¸åŒç±»å‹çš„å†…å®¹ä½¿ç”¨ä¸åŒçš„åˆ‡åˆ†ç­–ç•¥

```python
def adaptive_chunking(document: Document) -> List[Document]:
    """æ ¹æ®æ–‡æ¡£ç‰¹å¾è‡ªé€‚åº”åˆ‡åˆ†"""
    
    content = document.page_content
    
    # 1. åˆ†ææ–‡æ¡£ç‰¹å¾
    avg_sentence_length = calculate_avg_sentence_length(content)
    has_tables = detect_tables(content)
    has_code = detect_code_blocks(content)
    
    # 2. é€‰æ‹©åˆ‡åˆ†ç­–ç•¥
    if has_tables or has_code:
        # è¡¨æ ¼å’Œä»£ç éœ€è¦ä¿æŒå®Œæ•´æ€§
        splitter = RecursiveSentenceSplitter(
            chunk_size=20,  # æ›´å¤§çš„ chunk
            chunk_overlap=5
        )
    elif avg_sentence_length > 30:
        # é•¿å¥å­ï¼ˆå¦‚æ³•å¾‹æ–‡æ¡£ï¼‰
        splitter = RecursiveSentenceSplitter(
            chunk_size=8,   # è¾ƒå°‘çš„å¥å­
            chunk_overlap=2
        )
    else:
        # æ™®é€šæ–‡æ¡£
        splitter = RecursiveSentenceSplitter(
            chunk_size=12,
            chunk_overlap=4
        )
    
    return splitter.split_documents([document])
```

---

### 2.7 é¿å…è¿‡å¤§/è¿‡å° Chunk çš„æœºåˆ¶

PIKE-RAG å†…ç½®äº†å¤šå±‚ä¿æŠ¤æœºåˆ¶ï¼š

#### æœºåˆ¶ 1: LLM åˆ‡åˆ†å™¨çš„æ™ºèƒ½è°ƒæ•´

```python
# åœ¨ LLMPoweredRecursiveSplitter ä¸­

if len(chunk) == 0:
    # å¦‚æœ LLM å»ºè®®çš„åˆ‡åˆ†å¯¼è‡´ç©º chunk
    # â†’ åˆå¹¶ä¸¤ä¸ª chunkï¼Œé‡æ–°å¤„ç†
    chunk_summary = next_summary
    chunks = [chunks[0] + chunks[1]] + chunks[2:]
    continue
```

#### æœºåˆ¶ 2: æœ€å°/æœ€å¤§é•¿åº¦é™åˆ¶

```python
class SafeRecursiveSplitter(RecursiveSentenceSplitter):
    def __init__(
        self,
        min_chunk_size: int = 100,   # æœ€å° chunk å¤§å°
        max_chunk_size: int = 2000,  # æœ€å¤§ chunk å¤§å°
        **kwargs
    ):
        super().__init__(**kwargs)
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
    
    def split_text(self, text: str) -> List[str]:
        chunks = super().split_text(text)
        
        # åå¤„ç†ï¼šåˆå¹¶è¿‡å°çš„ chunk
        safe_chunks = []
        buffer = ""
        
        for chunk in chunks:
            if len(chunk) < self.min_chunk_size:
                buffer += " " + chunk
            else:
                if buffer:
                    safe_chunks.append(buffer.strip())
                    buffer = ""
                safe_chunks.append(chunk)
        
        if buffer:
            if safe_chunks:
                safe_chunks[-1] += " " + buffer
            else:
                safe_chunks.append(buffer)
        
        # åå¤„ç†ï¼šæ‹†åˆ†è¿‡å¤§çš„ chunk
        final_chunks = []
        for chunk in safe_chunks:
            if len(chunk) > self.max_chunk_size:
                # å¼ºåˆ¶æ‹†åˆ†
                sub_chunks = [
                    chunk[i:i+self.max_chunk_size] 
                    for i in range(0, len(chunk), self.max_chunk_size)
                ]
                final_chunks.extend(sub_chunks)
            else:
                final_chunks.append(chunk)
        
        return final_chunks
```

#### æœºåˆ¶ 3: è´¨é‡æ£€æŸ¥å’Œæ—¥å¿—

```python
# åœ¨åˆ‡åˆ†åè¿›è¡Œè´¨é‡æ£€æŸ¥

def validate_chunks(chunks: List[Document]) -> List[Document]:
    for i, chunk in enumerate(chunks):
        content_len = len(chunk.page_content)
        
        if content_len < 50:
            logger.warning(f"Chunk {i} too small: {content_len} chars")
        
        if content_len > 2000:
            logger.warning(f"Chunk {i} too large: {content_len} chars")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„å¥å­
        if not chunk.page_content.strip().endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')):
            logger.info(f"Chunk {i} may have incomplete sentence")
    
    return chunks
```

---

## ğŸ¯ Part 3: å®æˆ˜æŒ‡å—

### 3.1 å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹

```bash
# Step 1: å‡†å¤‡æ–‡æ¡£
mkdir -p data/raw_documents
# å°†å„ç§æ ¼å¼çš„æ–‡æ¡£æ”¾å…¥è¯¥ç›®å½•

# Step 2: é…ç½®åˆ‡åˆ†ç­–ç•¥
vim examples/my_project/configs/chunking.yml

# Step 3: è¿è¡Œåˆ‡åˆ†
python examples/chunking.py examples/my_project/configs/chunking.yml

# è¾“å‡º: data/my_project/chunks/
```

---

### 3.2 chunking.yml é…ç½®æ¨¡æ¿

```yaml
# ç¯å¢ƒå˜é‡
dotenv_path: .env

# æ—¥å¿—è®¾ç½®
log_root_dir: logs/my_project
experiment_name: chunking

# è¾“å…¥è¾“å‡ºè®¾ç½®
input_doc_setting:
  doc_dir: data/raw_documents
  extensions: [".txt", ".md", ".pdf", ".docx", ".xlsx"]

output_doc_setting:
  doc_dir: data/my_project/chunks
  suffix: pkl  # æˆ– jsonl

# LLM è®¾ç½®
llm_client:
  module_path: pikerag.llm_client
  class_name: AzureOpenAIClient
  args: {}
  
  llm_config:
    model: gpt-4
    temperature: 0
  
  cache_config:
    location_prefix: null
    auto_dump: True

# åˆ‡åˆ†å™¨è®¾ç½®
chunking_protocol:
  module_path: pikerag.prompts.chunking
  chunk_summary: chunk_summary_protocol
  chunk_summary_refinement: chunk_summary_refinement_protocol
  chunk_resplit: chunk_resplit_protocol

splitter:
  module_path: pikerag.document_transformers
  class_name: LLMPoweredRecursiveSplitter
  args:
    separators: ["\n\n", "\n", ". ", " ", ""]
    is_separator_regex: False
    chunk_size: 1000        # æ ¹æ®æ–‡æ¡£ç±»å‹è°ƒæ•´
    chunk_overlap: 200      # é‡å éƒ¨åˆ†
```

**æˆ–è€…ä½¿ç”¨ç®€å•çš„åŸºäºå¥å­çš„åˆ‡åˆ†ï¼š**

```yaml
splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: "en"              # "en" æˆ– "zh"
    chunk_size: 12          # æ¯ä¸ª chunk çš„å¥å­æ•°é‡
    chunk_overlap: 4        # å¥å­é‡å æ•°é‡
    num_parallel: 4         # å¹¶è¡Œå¤„ç†æ•°é‡
```

---

### 3.3 ä¸åŒæ–‡æ¡£ç±»å‹çš„æ¨èé…ç½®

#### å­¦æœ¯è®ºæ–‡ï¼ˆPDFï¼‰

```yaml
splitter:
  module_path: pikerag.document_transformers
  class_name: LLMPoweredRecursiveSplitter
  args:
    chunk_size: 1500      # è®ºæ–‡æ®µè½é€šå¸¸è¾ƒé•¿
    chunk_overlap: 300
    separators: ["\n\n", "\n", ". ", " "]
```

**åŸå› ï¼š**
- è®ºæ–‡åŒ…å«å®Œæ•´çš„é€»è¾‘æ®µè½
- éœ€è¦è¾ƒå¤§çš„ chunk ä¿æŒè®ºè¯å®Œæ•´æ€§
- è¾ƒå¤§çš„ overlap ä¿æŒä¸Šä¸‹æ–‡è¿è´¯

---

#### æŠ€æœ¯æ–‡æ¡£ï¼ˆMarkdownï¼‰

```yaml
splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: "en"
    chunk_size: 10        # æŠ€æœ¯æ–‡æ¡£å¥å­è¾ƒçŸ­
    chunk_overlap: 3
```

**åŸå› ï¼š**
- Markdown ç»“æ„æ¸…æ™°
- ä»£ç å—å’Œåˆ—è¡¨éœ€è¦ä¿æŒå®Œæ•´
- åŸºäºå¥å­çš„åˆ‡åˆ†æ›´åˆé€‚

---

#### åŒ»ç–—è®°å½•ï¼ˆWord/Excelï¼‰

```yaml
splitter:
  module_path: pikerag.document_transformers
  class_name: LLMPoweredRecursiveSplitter
  args:
    chunk_size: 800       # è®°å½•é€šå¸¸ç®€æ´
    chunk_overlap: 150
```

**åŸå› ï¼š**
- åŒ»ç–—æœ¯è¯­å¯†é›†
- éœ€è¦ LLM ç†è§£ä¸“ä¸šæœ¯è¯­
- ä¸­ç­‰å¤§å°çš„ chunk é€‚åˆç²¾ç¡®æ£€ç´¢

---

#### å®¢æˆ·æœåŠ¡å¯¹è¯ï¼ˆTXTï¼‰

```yaml
splitter:
  module_path: pikerag.document_transformers
  class_name: RecursiveSentenceSplitter
  args:
    lang: "zh"            # ä¸­æ–‡å®¢æœ
    chunk_size: 8
    chunk_overlap: 2
```

**åŸå› ï¼š**
- å¯¹è¯ç»“æ„ç®€å•
- å¥å­çº§åˆ«åˆ‡åˆ†ä¿æŒå¯¹è¯å®Œæ•´æ€§
- è¾ƒå°çš„ chunk æé«˜æ£€ç´¢ç²¾åº¦

---

### 3.4 å®æˆ˜æ¡ˆä¾‹ï¼šå¤„ç†æ··åˆæ–‡æ¡£é›†

å‡è®¾æ‚¨æœ‰ä»¥ä¸‹æ–‡æ¡£ï¼š
```
data/documents/
â”œâ”€â”€ papers/          # PDF å­¦æœ¯è®ºæ–‡
â”œâ”€â”€ manuals/         # Word æ“ä½œæ‰‹å†Œ
â”œâ”€â”€ transcripts/     # TXT ä¼šè®®è®°å½•
â””â”€â”€ spreadsheets/    # Excel æ•°æ®è¡¨
```

**ç­–ç•¥ 1: åˆ†åˆ«å¤„ç†ï¼ˆæ¨èï¼‰**

```bash
# ä¸ºæ¯ç§ç±»å‹åˆ›å»ºé…ç½®
python examples/chunking.py configs/chunking_papers.yml
python examples/chunking.py configs/chunking_manuals.yml
python examples/chunking.py configs/chunking_transcripts.yml
python examples/chunking.py configs/chunking_spreadsheets.yml
```

æ¯ä¸ªé…ç½®æ–‡ä»¶ä½¿ç”¨ä¸åŒçš„ `chunk_size` å’Œ `chunk_overlap`ã€‚

**ç­–ç•¥ 2: ç»Ÿä¸€å¤„ç†**

```yaml
input_doc_setting:
  doc_dir: data/documents
  extensions: [".pdf", ".docx", ".txt", ".xlsx"]

splitter:
  class_name: LLMPoweredRecursiveSplitter
  args:
    chunk_size: 1000      # ä¸­ç­‰å¤§å°
    chunk_overlap: 200
```

ä½¿ç”¨ä¸­ç­‰å‚æ•°ï¼Œè®© LLM è‡ªé€‚åº”è°ƒæ•´ã€‚

---

## ğŸ¯ Part 4: é«˜çº§æŠ€å·§

### 4.1 è‡ªé€‚åº” Chunk å¤§å°

æ ¹æ®æ–‡æ¡£å¤æ‚åº¦åŠ¨æ€è°ƒæ•´ï¼š

```python
def adaptive_chunk_size(document: Document) -> int:
    """æ ¹æ®æ–‡æ¡£ç‰¹å¾è‡ªé€‚åº”ç¡®å®š chunk å¤§å°"""
    content = document.page_content
    
    # ç‰¹å¾ 1: å¹³å‡å¥å­é•¿åº¦
    sentences = content.split('.')
    avg_sentence_len = sum(len(s) for s in sentences) / len(sentences)
    
    # ç‰¹å¾ 2: ä¸“ä¸šæœ¯è¯­å¯†åº¦
    technical_terms = count_technical_terms(content)
    term_density = technical_terms / len(content.split())
    
    # ç‰¹å¾ 3: æ–‡æ¡£ç±»å‹
    doc_type = document.metadata.get('type', 'unknown')
    
    # å†³ç­–é€»è¾‘
    if doc_type == 'academic':
        base_size = 1500
    elif doc_type == 'dialogue':
        base_size = 600
    else:
        base_size = 1000
    
    # æ ¹æ®å¥å­é•¿åº¦è°ƒæ•´
    if avg_sentence_len > 50:
        base_size *= 1.2
    
    # æ ¹æ®æœ¯è¯­å¯†åº¦è°ƒæ•´
    if term_density > 0.15:
        base_size *= 0.8  # æœ¯è¯­å¯†é›†æ—¶ç”¨å° chunk
    
    return int(base_size)
```

---

### 4.2 ä¿ç•™æ–‡æ¡£ç»“æ„

å¯¹äºç»“æ„åŒ–æ–‡æ¡£ï¼ˆå¦‚ Markdownã€HTMLï¼‰ï¼Œä¿ç•™ç»“æ„ä¿¡æ¯ï¼š

```python
class StructureAwareLoader(UnstructuredMarkdownLoader):
    def load(self) -> List[Document]:
        docs = super().load()
        
        # è§£æ Markdown ç»“æ„
        for doc in docs:
            content = doc.page_content
            
            # æå–æ ‡é¢˜å±‚çº§
            if content.startswith('# '):
                doc.metadata['heading_level'] = 1
                doc.metadata['heading'] = content.split('\n')[0][2:]
            elif content.startswith('## '):
                doc.metadata['heading_level'] = 2
                doc.metadata['heading'] = content.split('\n')[0][3:]
            
            # æ ‡è®°ä»£ç å—
            if '```' in content:
                doc.metadata['contains_code'] = True
        
        return docs
```

åœ¨åˆ‡åˆ†æ—¶è€ƒè™‘ç»“æ„ï¼š

```python
def structure_aware_split(docs: List[Document], splitter) -> List[Document]:
    chunks = []
    
    for doc in docs:
        # ä»£ç å—ä¸åˆ‡åˆ†
        if doc.metadata.get('contains_code', False):
            chunks.append(doc)
        else:
            # æ­£å¸¸åˆ‡åˆ†
            chunks.extend(splitter.split_documents([doc]))
    
    return chunks
```

---

### 4.3 å¤šæ¨¡æ€å†…å®¹å¤„ç†

å¯¹äºåŒ…å«å›¾åƒå’Œè¡¨æ ¼çš„æ–‡æ¡£ï¼š

```python
from unstructured.partition.pdf import partition_pdf

def extract_multimodal_content(pdf_path: str) -> List[Document]:
    """æå– PDF ä¸­çš„æ–‡æœ¬ã€å›¾åƒå’Œè¡¨æ ¼"""
    
    # ä½¿ç”¨ unstructured åº“è§£æ
    elements = partition_pdf(
        filename=pdf_path,
        extract_images_in_pdf=True,
        infer_table_structure=True,
        strategy="hi_res"  # é«˜åˆ†è¾¨ç‡æ¨¡å¼
    )
    
    documents = []
    
    for element in elements:
        # æ–‡æœ¬
        if element.category == "Text":
            doc = Document(
                page_content=element.text,
                metadata={"type": "text", "page": element.metadata.page_number}
            )
            documents.append(doc)
        
        # è¡¨æ ¼
        elif element.category == "Table":
            # å°†è¡¨æ ¼è½¬æ¢ä¸º Markdown æ ¼å¼
            table_markdown = element.metadata.text_as_html  # æˆ–è‡ªå®šä¹‰æ ¼å¼
            doc = Document(
                page_content=f"[TABLE]\n{table_markdown}\n[/TABLE]",
                metadata={
                    "type": "table", 
                    "page": element.metadata.page_number
                }
            )
            documents.append(doc)
        
        # å›¾åƒï¼ˆä¿å­˜æè¿°æˆ– OCR ç»“æœï¼‰
        elif element.category == "Image":
            # å¯ä»¥ä½¿ç”¨ Vision LLM ç”Ÿæˆå›¾åƒæè¿°
            image_description = generate_image_caption(element)
            doc = Document(
                page_content=f"[IMAGE] {image_description}",
                metadata={
                    "type": "image",
                    "page": element.metadata.page_number,
                    "image_path": element.metadata.image_path
                }
            )
            documents.append(doc)
    
    return documents
```

---

### 4.4 å¤„ç†è¶…é•¿æ–‡æ¡£

å¯¹äºéå¸¸é•¿çš„æ–‡æ¡£ï¼ˆå¦‚å®Œæ•´çš„ä¹¦ç±ï¼‰ï¼š

```python
class HierarchicalChunker:
    """åˆ†å±‚åˆ‡åˆ†ï¼šç« èŠ‚ -> æ®µè½ -> å¥å­"""
    
    def __init__(self, chapter_splitter, paragraph_splitter):
        self.chapter_splitter = chapter_splitter
        self.paragraph_splitter = paragraph_splitter
    
    def split(self, document: Document) -> List[Document]:
        # Level 1: æŒ‰ç« èŠ‚åˆ‡åˆ†
        chapters = self.chapter_splitter.split_documents([document])
        
        all_chunks = []
        for chapter in chapters:
            # Level 2: æ¯ä¸ªç« èŠ‚æŒ‰æ®µè½åˆ‡åˆ†
            paragraphs = self.paragraph_splitter.split_documents([chapter])
            
            # ä¿ç•™ç« èŠ‚ä¿¡æ¯
            for para in paragraphs:
                para.metadata['chapter'] = chapter.metadata.get('heading', 'Unknown')
                all_chunks.append(para)
        
        return all_chunks
```

---

## ğŸ”§ Part 5: æ•…éšœæ’é™¤

### é—®é¢˜ 1: åˆ‡åˆ†åçš„ Chunk å¤ªå°

**ç—‡çŠ¶ï¼š**
```
Chunk 1: "The"
Chunk 2: "quick brown fox"
Chunk 3: "jumps"
```

**åŸå› ï¼š**
- `chunk_size` è®¾ç½®è¿‡å°
- æ–‡æ¡£æœ¬èº«ç»“æ„ç ´ç¢

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# æ–¹æ³• 1: å¢åŠ  chunk_size
splitter_args:
  chunk_size: 2000  # ä» 500 å¢åŠ åˆ° 2000

# æ–¹æ³• 2: åå¤„ç†åˆå¹¶å° chunks
def merge_small_chunks(chunks: List[Document], min_size: int = 200) -> List[Document]:
    merged = []
    buffer = ""
    buffer_meta = {}
    
    for chunk in chunks:
        if len(chunk.page_content) < min_size:
            buffer += " " + chunk.page_content
            buffer_meta = chunk.metadata
        else:
            if buffer:
                merged.append(Document(page_content=buffer.strip(), metadata=buffer_meta))
                buffer = ""
            merged.append(chunk)
    
    if buffer:
        merged.append(Document(page_content=buffer.strip(), metadata=buffer_meta))
    
    return merged
```

---

### é—®é¢˜ 2: åˆ‡åˆ†åçš„ Chunk å¤ªå¤§

**ç—‡çŠ¶ï¼š**
```
Chunk 1: 5000 å­—çš„é•¿æ®µè½
Chunk 2: 8000 å­—çš„é•¿æ®µè½
```

**åŸå› ï¼š**
- æ–‡æ¡£æ²¡æœ‰æ˜æ˜¾çš„åˆ†éš”ç¬¦
- `chunk_size` è®¾ç½®è¿‡å¤§

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# æ–¹æ³• 1: å‡å° chunk_size å¹¶å¢åŠ åˆ†éš”ç¬¦
splitter_args:
  chunk_size: 800
  separators: ["\n\n", "\n", ". ", ", ", " ", ""]  # æ›´ç»†ç²’åº¦

# æ–¹æ³• 2: å¼ºåˆ¶æ‹†åˆ†è¿‡å¤§çš„ chunks
def split_large_chunks(chunks: List[Document], max_size: int = 2000) -> List[Document]:
    result = []
    
    for chunk in chunks:
        if len(chunk.page_content) > max_size:
            # å¼ºåˆ¶æŒ‰å¥å­æ‹†åˆ†
            sentences = chunk.page_content.split('.')
            
            current = ""
            for sent in sentences:
                if len(current) + len(sent) < max_size:
                    current += sent + "."
                else:
                    result.append(Document(
                        page_content=current.strip(),
                        metadata=chunk.metadata
                    ))
                    current = sent + "."
            
            if current:
                result.append(Document(
                    page_content=current.strip(),
                    metadata=chunk.metadata
                ))
        else:
            result.append(chunk)
    
    return result
```

---

### é—®é¢˜ 3: PDF æå–å¤±è´¥æˆ–ä¹±ç 

**ç—‡çŠ¶ï¼š**
```
Error: Unable to extract text from PDF
æˆ–
Chunk content: "ãŠ£ãŠ¤ãŠ¥ãŠ¦ãŠ§ãŠ¨"
```

**åŸå› ï¼š**
- PDF æ˜¯æ‰«æç‰ˆï¼ˆå›¾åƒï¼‰
- PDF ä½¿ç”¨éæ ‡å‡†ç¼–ç 

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# æ–¹æ³• 1: ä½¿ç”¨ OCR
from langchain_community.document_loaders import UnstructuredPDFLoader

loader = UnstructuredPDFLoader(
    file_path="document.pdf",
    mode="elements",
    strategy="hi_res",  # ä½¿ç”¨é«˜ç²¾åº¦æ¨¡å¼
    ocr_languages="eng+chi_sim"  # è‹±æ–‡ + ç®€ä½“ä¸­æ–‡
)

# æ–¹æ³• 2: å°è¯•å¤šç§ PDF åº“
def load_pdf_with_fallback(pdf_path: str) -> List[Document]:
    try:
        # å°è¯• PyPDF
        from langchain_community.document_loaders import PyPDFLoader
        return PyPDFLoader(pdf_path).load()
    except:
        pass
    
    try:
        # å°è¯• Unstructured
        from langchain_community.document_loaders import UnstructuredPDFLoader
        return UnstructuredPDFLoader(pdf_path).load()
    except:
        pass
    
    try:
        # å°è¯• PDFMiner
        from langchain_community.document_loaders import PDFMinerLoader
        return PDFMinerLoader(pdf_path).load()
    except:
        raise Exception(f"Failed to load PDF: {pdf_path}")
```

---

### é—®é¢˜ 4: LLM åˆ‡åˆ†é€Ÿåº¦å¤ªæ…¢

**ç—‡çŠ¶ï¼š**
å¤„ç† 100 ä¸ªæ–‡æ¡£éœ€è¦ 2 å°æ—¶

**åŸå› ï¼š**
- LLM API è°ƒç”¨é¢‘ç¹
- æ²¡æœ‰ä½¿ç”¨ç¼“å­˜

**è§£å†³æ–¹æ¡ˆï¼š**

```yaml
# æ–¹æ³• 1: ç¡®ä¿ç¼“å­˜å¼€å¯
llm_client:
  cache_config:
    auto_dump: True  # è‡ªåŠ¨ä¿å­˜ç¼“å­˜

# æ–¹æ³• 2: ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
llm_config:
  model: gpt-35-turbo  # è€Œä¸æ˜¯ gpt-4

# æ–¹æ³• 3: å‡å°‘ LLM è°ƒç”¨
splitter:
  class_name: RecursiveSentenceSplitter  # ä¸ä½¿ç”¨ LLM
```

**æˆ–è€…æ‰¹å¤„ç†ï¼š**

```python
# å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡æ¡£
from concurrent.futures import ThreadPoolExecutor

def process_document(doc_path: str):
    loader = get_loader(doc_path)
    docs = loader.load()
    chunks = splitter.split_documents(docs)
    save_chunks(chunks, output_path)

with ThreadPoolExecutor(max_workers=4) as executor:
    executor.map(process_document, document_paths)
```

---

### é—®é¢˜ 5: è¡¨æ ¼å†…å®¹ä¸¢å¤±

**ç—‡çŠ¶ï¼š**
Excel æˆ– PDF ä¸­çš„è¡¨æ ¼å˜æˆä¹±ç æˆ–ä¸¢å¤±

**è§£å†³æ–¹æ¡ˆï¼š**

```python
# å¯¹äº Excel
from langchain_community.document_loaders import UnstructuredExcelLoader

loader = UnstructuredExcelLoader(
    file_path="data.xlsx",
    mode="elements"  # ä¿ç•™å…ƒç´ ç±»å‹
)

docs = loader.load()

# å•ç‹¬å¤„ç†è¡¨æ ¼å…ƒç´ 
for doc in docs:
    if doc.metadata.get('category') == 'Table':
        # ä¿ç•™è¡¨æ ¼ç»“æ„
        doc.metadata['preserve_structure'] = True
        # åœ¨åˆ‡åˆ†æ—¶è·³è¿‡è¯¥ chunk
```

---

## ğŸ“Š Part 6: æ€§èƒ½ä¼˜åŒ–

### 6.1 åˆ‡åˆ†è´¨é‡è¯„ä¼°

```python
def evaluate_chunking_quality(chunks: List[Document]) -> dict:
    """è¯„ä¼°åˆ‡åˆ†è´¨é‡"""
    
    # ç»Ÿè®¡ä¿¡æ¯
    chunk_sizes = [len(chunk.page_content) for chunk in chunks]
    
    metrics = {
        "num_chunks": len(chunks),
        "avg_size": sum(chunk_sizes) / len(chunk_sizes),
        "min_size": min(chunk_sizes),
        "max_size": max(chunk_sizes),
        "std_size": np.std(chunk_sizes),
        
        # è´¨é‡æŒ‡æ ‡
        "too_small_ratio": sum(1 for s in chunk_sizes if s < 200) / len(chunks),
        "too_large_ratio": sum(1 for s in chunk_sizes if s > 2000) / len(chunks),
        "optimal_ratio": sum(1 for s in chunk_sizes if 500 <= s <= 1500) / len(chunks),
    }
    
    return metrics

# ä½¿ç”¨
metrics = evaluate_chunking_quality(chunks)
print(f"å¹³å‡å¤§å°: {metrics['avg_size']:.0f} å­—ç¬¦")
print(f"æœ€ä¼˜æ¯”ä¾‹: {metrics['optimal_ratio']:.1%}")
```

---

### 6.2 A/B æµ‹è¯•ä¸åŒç­–ç•¥

```python
strategies = [
    {
        "name": "Small Chunks",
        "chunk_size": 500,
        "chunk_overlap": 100
    },
    {
        "name": "Medium Chunks",
        "chunk_size": 1000,
        "chunk_overlap": 200
    },
    {
        "name": "Large Chunks",
        "chunk_size": 1500,
        "chunk_overlap": 300
    }
]

results = {}
for strategy in strategies:
    # ä½¿ç”¨è¯¥ç­–ç•¥åˆ‡åˆ†
    splitter = RecursiveCharacterTextSplitter(**strategy)
    chunks = splitter.split_documents(docs)
    
    # è¯„ä¼°
    metrics = evaluate_chunking_quality(chunks)
    
    # åœ¨å®é™… QA ä»»åŠ¡ä¸Šæµ‹è¯•
    qa_metrics = run_qa_evaluation(chunks)
    
    results[strategy['name']] = {
        "chunking_metrics": metrics,
        "qa_performance": qa_metrics
    }

# é€‰æ‹©æœ€ä½³ç­–ç•¥
best_strategy = max(results, key=lambda k: results[k]['qa_performance']['f1'])
print(f"æœ€ä½³ç­–ç•¥: {best_strategy}")
```

---

## ğŸ“ Part 7: æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ–‡æ¡£åŠ è½½è‡ªåŠ¨åŒ–**
   - PIKE-RAG æ”¯æŒ 6+ ç§ä¸»æµæ ¼å¼
   - åŸºäºæ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹© Loader
   - å¯æ‰©å±•æ”¯æŒè‡ªå®šä¹‰æ ¼å¼

2. **æ™ºèƒ½åˆ‡åˆ†ç­–ç•¥**
   - **LLM é©±åŠ¨åˆ‡åˆ†**ï¼šè¯­ä¹‰å®Œæ•´æ€§é«˜ï¼Œé€‚åˆå¤æ‚æ–‡æ¡£
   - **åŸºäºå¥å­åˆ‡åˆ†**ï¼šå¿«é€Ÿé«˜æ•ˆï¼Œé€‚åˆç»“æ„åŒ–æ–‡æ¡£
   - **æ··åˆç­–ç•¥**ï¼šæ ¹æ®æ–‡æ¡£ç±»å‹çµæ´»é€‰æ‹©

3. **Chunk å¤§å°æ§åˆ¶**
   - ä½¿ç”¨ `chunk_size` å’Œ `chunk_overlap` å‚æ•°
   - LLM è‡ªåŠ¨è°ƒæ•´è¯­ä¹‰è¾¹ç•Œ
   - åå¤„ç†æœºåˆ¶é˜²æ­¢è¿‡å¤§/è¿‡å°

4. **æœ€ä½³å®è·µ**
   - å­¦æœ¯è®ºæ–‡ï¼š1200-1500 å­—ç¬¦
   - æŠ€æœ¯æ–‡æ¡£ï¼š800-1000 å­—ç¬¦
   - å¯¹è¯è®°å½•ï¼š500-800 å­—ç¬¦
   - æ ¹æ®å®é™…æ•ˆæœè°ƒä¼˜

### æ¨èé…ç½®å†³ç­–æ ‘

```
æ–‡æ¡£ç±»å‹ï¼Ÿ
â”œâ”€ ç»“æ„åŒ–ï¼ˆMarkdown, ä»£ç ï¼‰
â”‚  â””â”€ ä½¿ç”¨ RecursiveSentenceSplitter
â”‚     chunk_size: 8-12 å¥å­
â”‚
â”œâ”€ ä¸“ä¸šé¢†åŸŸï¼ˆåŒ»ç–—ã€æ³•å¾‹ï¼‰
â”‚  â””â”€ ä½¿ç”¨ LLMPoweredRecursiveSplitter
â”‚     chunk_size: 800-1000
â”‚
â”œâ”€ å­¦æœ¯è®ºæ–‡ï¼ˆPDFï¼‰
â”‚  â””â”€ ä½¿ç”¨ LLMPoweredRecursiveSplitter
â”‚     chunk_size: 1200-1500
â”‚
â””â”€ æ··åˆ/æœªçŸ¥
   â””â”€ ä½¿ç”¨ LLMPoweredRecursiveSplitter
      chunk_size: 1000
```

---

## ğŸš€ Part 8: ä¸‹ä¸€æ­¥

ç°åœ¨æ‚¨å·²ç»æŒæ¡äº†æ–‡æ¡£å¤„ç†å’Œæ™ºèƒ½åˆ‡åˆ†ï¼Œå¯ä»¥ç»§ç»­å­¦ä¹ ï¼š

1. **Atom æå–ï¼ˆTaggingï¼‰**
   - å¦‚ä½•ä» Chunks æå–åŸå­é—®é¢˜
   - æç¤ºå·¥ç¨‹ä¼˜åŒ–

2. **å‘é‡æ•°æ®åº“æ„å»º**
   - Chunk å’Œ Atom çš„åŒå‘é‡å­˜å‚¨
   - Embedding æ¨¡å‹é€‰æ‹©

3. **é—®é¢˜åˆ†è§£å·¥ä½œæµ**
   - å¦‚ä½•åˆ©ç”¨åˆ‡åˆ†å¥½çš„çŸ¥è¯†è¿›è¡Œå¤šè·³æ¨ç†

---

## ğŸ“š ç›¸å…³æ–‡ä»¶ç´¢å¼•

- **æ–‡æ¡£åŠ è½½**: `pikerag/document_loaders/utils.py`
- **LLM åˆ‡åˆ†å™¨**: `pikerag/document_transformers/splitter/llm_powered_recursive_splitter.py`
- **å¥å­åˆ‡åˆ†å™¨**: `pikerag/document_transformers/splitter/recursive_sentence_splitter.py`
- **åˆ‡åˆ†æç¤º**: `pikerag/prompts/chunking/recursive_splitter.py`
- **åˆ‡åˆ†å·¥ä½œæµ**: `pikerag/workflows/chunking.py`
- **é…ç½®ç¤ºä¾‹**: `examples/biology/configs/chunking.yml`

---

## â“ FAQ

**Q: æˆ‘åº”è¯¥ä½¿ç”¨å¤šå¤§çš„ chunk_sizeï¼Ÿ**

A: å–å†³äºæ‚¨çš„åº”ç”¨ï¼š
- **æ£€ç´¢ä¸ºä¸»**ï¼ˆå¦‚ç®€å•é—®ç­”ï¼‰ï¼š500-800 å­—ç¬¦ï¼ˆæ›´ç²¾ç¡®ï¼‰
- **ç†è§£ä¸ºä¸»**ï¼ˆå¦‚æ‘˜è¦ç”Ÿæˆï¼‰ï¼š1200-1500 å­—ç¬¦ï¼ˆæ›´å®Œæ•´ï¼‰
- **å»ºè®®**ï¼šä» 1000 å¼€å§‹ï¼Œæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´

**Q: LLM åˆ‡åˆ†å’Œå¥å­åˆ‡åˆ†å“ªä¸ªå¥½ï¼Ÿ**

A: 
- **LLM åˆ‡åˆ†**ï¼šè´¨é‡æ›´é«˜ï¼Œä½†é€Ÿåº¦æ…¢ã€æˆæœ¬é«˜
- **å¥å­åˆ‡åˆ†**ï¼šå¿«é€Ÿä¾¿å®œï¼Œä½†å¯èƒ½åœ¨è¯­ä¹‰è¾¹ç•Œå¤„åˆ‡åˆ†ä¸ç†æƒ³
- **å»ºè®®**ï¼šå…ˆç”¨å¥å­åˆ‡åˆ†å¿«é€ŸåŸå‹ï¼Œæ•ˆæœä¸ç†æƒ³å†ç”¨ LLM

**Q: å¦‚ä½•å¤„ç†ä¸­è‹±æ–‡æ··åˆæ–‡æ¡£ï¼Ÿ**

A:
```python
# ä½¿ç”¨æ”¯æŒå¤šè¯­è¨€çš„ Spacy æ¨¡å‹
splitter = RecursiveSentenceSplitter(
    lang="zh",  # ä¸»è¦è¯­è¨€
    chunk_size=10
)
```

æˆ–ä½¿ç”¨ LLM åˆ‡åˆ†ï¼Œå®ƒè‡ªåŠ¨å¤„ç†å¤šè¯­è¨€ã€‚

**Q: Chunk å¤ªå°æˆ–å¤ªå¤§ä¼šæ€æ ·ï¼Ÿ**

A:
- **å¤ªå°**ï¼šä¸Šä¸‹æ–‡ä¸è¶³ï¼ŒLLM æ— æ³•å‡†ç¡®å›ç­”
- **å¤ªå¤§**ï¼šåŒ…å«å™ªéŸ³ï¼Œæ£€ç´¢ä¸ç²¾ç¡®ï¼ŒToken æµªè´¹
- **å¹³è¡¡**ï¼šæ ¹æ®æ‚¨çš„ LLM context window å’Œä»»åŠ¡éœ€æ±‚è°ƒæ•´

---

å¸Œæœ›è¿™ä»½æ–‡æ¡£èƒ½å¸®åŠ©æ‚¨ç†è§£ PIKE-RAG çš„æ–‡æ¡£å¤„ç†æœºåˆ¶ï¼å¦‚æœ‰ç–‘é—®ï¼Œè¯·æŸ¥é˜…ç›¸å…³ä»£ç æˆ–æé—®ã€‚
