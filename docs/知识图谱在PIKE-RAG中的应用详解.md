# çŸ¥è¯†å›¾è°±åœ¨ PIKE-RAG ä¸­çš„åº”ç”¨è¯¦è§£

## ğŸ“š ç›®å½•
- [1. æ¦‚è¿°](#1-æ¦‚è¿°)
- [2. PIKE-RAG ä¸­çš„çŸ¥è¯†å›¾è°±æ¦‚å¿µ](#2-pike-rag-ä¸­çš„çŸ¥è¯†å›¾è°±æ¦‚å¿µ)
- [3. å¼‚æ„çŸ¥è¯†å›¾è°±çš„æ•°æ®ç»“æ„](#3-å¼‚æ„çŸ¥è¯†å›¾è°±çš„æ•°æ®ç»“æ„)
- [4. çŸ¥è¯†å›¾è°±çš„æ„å»ºæµç¨‹](#4-çŸ¥è¯†å›¾è°±çš„æ„å»ºæµç¨‹)
- [5. çŸ¥è¯†å›¾è°±çš„æ£€ç´¢æœºåˆ¶](#5-çŸ¥è¯†å›¾è°±çš„æ£€ç´¢æœºåˆ¶)
- [6. æ•°æ®æµè½¬è¯¦è§£](#6-æ•°æ®æµè½¬è¯¦è§£)
- [7. ä»£ç å®ç°è§£æ](#7-ä»£ç å®ç°è§£æ)
- [8. é…ç½®ç¤ºä¾‹](#8-é…ç½®ç¤ºä¾‹)
- [9. æ€»ç»“ä¸æœ€ä½³å®è·µ](#9-æ€»ç»“ä¸æœ€ä½³å®è·µ)

---

## 1. æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ PIKE-RAG çš„çŸ¥è¯†å›¾è°±ï¼Ÿ

PIKE-RAG é‡‡ç”¨äº†ä¸€ç§åˆ›æ–°çš„**å¼‚æ„çŸ¥è¯†å›¾è°±ï¼ˆHeterogeneous Knowledge Graphï¼‰**è®¾è®¡ï¼Œä¸ä¼ ç»Ÿçš„å®ä½“-å…³ç³»å›¾è°±ä¸åŒï¼Œå®ƒé€šè¿‡**åŒå±‚çŸ¥è¯†è¡¨ç¤ºç»“æ„**æ¥ç»„ç»‡å’Œæ£€ç´¢ä¿¡æ¯ï¼š

```
ä¼ ç»ŸçŸ¥è¯†å›¾è°±:                    PIKE-RAG å¼‚æ„çŸ¥è¯†å›¾è°±:
                                 
å®ä½“ --å…³ç³»--> å®ä½“               Chunk (ç²—ç²’åº¦) â†â†’ Atom (ç»†ç²’åº¦)
   \                                  |                    |
    \--å…³ç³»--> å®ä½“                  å®Œæ•´æ–‡æ¡£ç‰‡æ®µ         åŸå­é—®é¢˜
                                     |                    |
                                  å‘é‡å­˜å‚¨              å‘é‡å­˜å‚¨
                                  Chunk Store          Atom Store
```

### 1.2 æ ¸å¿ƒä¼˜åŠ¿

1. **å¤šç²’åº¦æ£€ç´¢**: ç»†ç²’åº¦æ£€ç´¢ï¼ˆAtomï¼‰+ ç²—ç²’åº¦è¿”å›ï¼ˆChunkï¼‰
2. **è¯­ä¹‰å¯¹é½**: åŸå­é—®é¢˜ï¼ˆAtomï¼‰ä¸ç”¨æˆ·é—®é¢˜åœ¨è¯­ä¹‰ä¸Šå¤©ç„¶å¯¹é½
3. **ä¸Šä¸‹æ–‡å®Œæ•´**: æœ€ç»ˆè¿”å›å®Œæ•´çš„ Chunkï¼Œä¿è¯ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸ä¸¢å¤±
4. **æ”¯æŒå¤šè·³æ¨ç†**: é€šè¿‡ Atom ä¹‹é—´çš„å…³è”å®ç°çŸ¥è¯†é“¾æ¥
5. **å¯è§£é‡Šæ€§å¼º**: æ¯ä¸ªæ£€ç´¢ç»“æœéƒ½å¯è¿½æº¯åˆ°å…·ä½“çš„åŸå­é—®é¢˜

---

## 2. PIKE-RAG ä¸­çš„çŸ¥è¯†å›¾è°±æ¦‚å¿µ

### 2.1 å¼‚æ„çŸ¥è¯†å›¾è°±çš„ä¸¤ä¸ªå±‚æ¬¡

#### å±‚æ¬¡ 1: Chunk å±‚ï¼ˆæ–‡æ¡£å—å±‚ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Chunk (æ–‡æ¡£å—)                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ å®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µï¼ˆ500-1000å­—ï¼‰              â”‚
â”‚ â€¢ åŒ…å«ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯                      â”‚
â”‚ â€¢ ç”¨äºæœ€ç»ˆæä¾›ç»™ LLM çš„å†…å®¹                 â”‚
â”‚ â€¢ æ¯ä¸ª Chunk æœ‰å”¯ä¸€ ID                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¤ºä¾‹ Chunk:
{
  "chunk_id": "chunk_001",
  "title": "2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡",
  "content": "ã€Šå¯„ç”Ÿè™«ã€‹(Parasite)æ˜¯ç”±éŸ©å›½å¯¼æ¼”å¥‰ä¿Šæ˜Šæ‰§å¯¼çš„2019å¹´é»‘è‰²å–œå‰§æƒŠæ‚šç‰‡ã€‚
             è¯¥ç‰‡åœ¨2020å¹´ç¬¬92å±Šå¥¥æ–¯å¡é‡‘åƒå¥–ä¸Šè·å¾—æœ€ä½³å½±ç‰‡ã€æœ€ä½³å¯¼æ¼”ã€æœ€ä½³åŸåˆ›å‰§æœ¬å’Œ
             æœ€ä½³å›½é™…å½±ç‰‡å››é¡¹å¤§å¥–ã€‚å¥‰ä¿Šæ˜Šå¯¼æ¼”1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚..."
}
```

#### å±‚æ¬¡ 2: Atom å±‚ï¼ˆåŸå­é—®é¢˜å±‚ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Atom (åŸå­é—®é¢˜)                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ä» Chunk ä¸­æå–çš„ç»†ç²’åº¦çŸ¥è¯†ç‚¹             â”‚
â”‚ â€¢ ä»¥"é—®é¢˜"å½¢å¼è¡¨ç¤º                          â”‚
â”‚ â€¢ æ¯ä¸ª Atom å…³è”åˆ°æº Chunk ID               â”‚
â”‚ â€¢ ç”¨äºç²¾ç¡®çš„è¯­ä¹‰æ£€ç´¢                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä»ä¸Šè¿° Chunk æå–çš„ Atoms:
1. "2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡æ˜¯å“ªéƒ¨ç”µå½±ï¼Ÿ"
2. "ã€Šå¯„ç”Ÿè™«ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ"
3. "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
4. "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªä¸€å¹´ï¼Ÿ"
5. "ã€Šå¯„ç”Ÿè™«ã€‹åœ¨å¥¥æ–¯å¡è·å¾—äº†å“ªäº›å¥–é¡¹ï¼Ÿ"
```

### 2.2 çŸ¥è¯†å›¾è°±çš„å›¾ç»“æ„

è™½ç„¶è¡¨é¢ä¸Šæ˜¯åŒå±‚å‘é‡å­˜å‚¨ï¼Œä½†å®é™…å½¢æˆäº†ä¸€ä¸ªå›¾ç»“æ„ï¼š

```
                    çŸ¥è¯†å›¾è°±ç»“æ„è§†å›¾

                [ç”¨æˆ·é—®é¢˜: "å¥‰ä¿Šæ˜Šå‡ºç”Ÿåœ°?"]
                           â†“
                    ã€æ£€ç´¢ Atom Storeã€‘
                           â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                  â†“                  â†“
   [Atom 3]           [Atom 7]           [Atom 12]
"å¥‰ä¿Šæ˜Šå‡ºç”Ÿåœ¨å“ªï¼Ÿ"  "å¥‰ä¿Šæ˜Šå›½ç±ï¼Ÿ"    "å¥‰ä¿Šæ˜Šæˆé•¿ç»å†ï¼Ÿ"
   (score: 0.95)      (score: 0.82)      (score: 0.78)
        â†“                  â†“                  â†“
   source_chunk_id    source_chunk_id    source_chunk_id
        â†“                  â†“                  â†“
   [Chunk 001] â†â”€â”€â”€â”€â”€[Chunk 001]       [Chunk 005]
        â†“
   ã€è¿”å›å®Œæ•´ Chunkã€‘
   "ã€Šå¯„ç”Ÿè™«ã€‹...å¥‰ä¿Šæ˜Š1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚..."
```

è¿™ç§ç»“æ„ä¸­ï¼š
- **èŠ‚ç‚¹**: Chunk å’Œ Atom æ˜¯ä¸¤ç±»ä¸åŒçš„èŠ‚ç‚¹
- **è¾¹**: Atom åˆ° Chunk çš„ `source_chunk_id` å…³ç³»
- **éšå¼å…³è”**: å±äºåŒä¸€ Chunk çš„ Atoms ä¹‹é—´æœ‰éšå¼å…³è”

---

## 3. å¼‚æ„çŸ¥è¯†å›¾è°±çš„æ•°æ®ç»“æ„

### 3.1 æ ¸å¿ƒæ•°æ®ç±»

#### AtomRetrievalInfo - åŸå­æ£€ç´¢ä¿¡æ¯

```python
@dataclass
class AtomRetrievalInfo:
    atom_query: str              # ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢
    atom: str                    # æ£€ç´¢åˆ°çš„åŸå­é—®é¢˜
    source_chunk_title: str      # æºæ–‡æ¡£å—çš„æ ‡é¢˜
    source_chunk: str            # æºæ–‡æ¡£å—çš„å®Œæ•´å†…å®¹
    source_chunk_id: str         # æºæ–‡æ¡£å—çš„ID (å…³é”®å…³è”å­—æ®µ)
    retrieval_score: float       # æ£€ç´¢ç›¸ä¼¼åº¦åˆ†æ•°
    atom_embedding: List[float]  # åŸå­é—®é¢˜çš„å‘é‡è¡¨ç¤º
```

**å…³é”®å­—æ®µè¯´æ˜**:
- `source_chunk_id`: è¿™æ˜¯ Atom å’Œ Chunk ä¹‹é—´çš„**æ ¸å¿ƒå…³è”å­—æ®µ**ï¼Œå½¢æˆå›¾è°±çš„"è¾¹"
- `atom`: åŸå­é—®é¢˜æœ¬èº«ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ä¸­çš„ç»†ç²’åº¦"èŠ‚ç‚¹"
- `source_chunk`: å®Œæ•´æ–‡æ¡£å†…å®¹ï¼Œæ˜¯çŸ¥è¯†å›¾è°±ä¸­çš„ç²—ç²’åº¦"èŠ‚ç‚¹"

### 3.2 åŒå‘é‡å­˜å‚¨æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ChunkAtomRetriever                          â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  _chunk_store      â”‚      â”‚  _atom_store        â”‚   â”‚
â”‚  â”‚  (Chroma)          â”‚      â”‚  (Chroma)           â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Document:          â”‚      â”‚ Document:           â”‚   â”‚
â”‚  â”‚ - page_content:    â”‚      â”‚ - page_content:     â”‚   â”‚
â”‚  â”‚     å®Œæ•´ Chunk å†…å®¹â”‚      â”‚     åŸå­é—®é¢˜æ–‡æœ¬    â”‚   â”‚
â”‚  â”‚ - metadata:        â”‚â—„â”€â”€â”€â”€â”€â”¤ - metadata:         â”‚   â”‚
â”‚  â”‚   - id             â”‚ å…³è” â”‚   - source_chunk_id â”‚   â”‚
â”‚  â”‚   - title          â”‚      â”‚   - title           â”‚   â”‚
â”‚  â”‚   - atom_questions â”‚      â”‚                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å­˜å‚¨è¯´æ˜**:

1. **_chunk_store (Chunk å‘é‡å­˜å‚¨)**
   ```python
   Document(
       page_content="å®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µå†…å®¹...",
       metadata={
           "id": "chunk_001",
           "title": "æ–‡æ¡£æ ‡é¢˜",
           "atom_questions_str": "é—®é¢˜1\né—®é¢˜2\né—®é¢˜3"
       }
   )
   ```

2. **_atom_store (Atom å‘é‡å­˜å‚¨)**
   ```python
   Document(
       page_content="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",  # å•ä¸ªåŸå­é—®é¢˜
       metadata={
           "source_chunk_id": "chunk_001",     # æŒ‡å‘æº Chunk
           "title": "2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡"
       }
   )
   ```

### 3.3 NetworkX å›¾å¢å¼ºï¼ˆå¯é€‰ï¼‰

è™½ç„¶å½“å‰å®ç°ä¸»è¦ä½¿ç”¨å‘é‡å­˜å‚¨ï¼Œä½† PIKE-RAG ä¿ç•™äº† NetworkX çš„å›¾éå†èƒ½åŠ›ï¼š

```python
class NetworkxMixin:
    def _get_subgraph_by_entity(
        self, 
        graph: nx.Graph, 
        entities: Iterable, 
        neighbor_layer: int = 1
    ) -> nx.Graph:
        """
        æ ¹æ®ç»™å®šçš„å®ä½“æå–å­å›¾ï¼ŒåŒ…å«æŒ‡å®šè·³æ•°å†…çš„é‚»å±…èŠ‚ç‚¹
        
        Args:
            graph: å®Œæ•´çš„çŸ¥è¯†å›¾è°±
            entities: èµ·å§‹å®ä½“é›†åˆ
            neighbor_layer: æ‰©å±•çš„é‚»å±…å±‚æ•°
        
        Returns:
            è¿‡æ»¤åçš„å­å›¾
        """
```

**ä½¿ç”¨åœºæ™¯**: 
- å¤šè·³æ¨ç†: ä»ä¸€ä¸ª Atom å‡ºå‘ï¼Œéå†å…¶ç›¸å…³çš„ Chunk å’Œå…¶ä»– Atoms
- å®ä½“å…³è”åˆ†æ: æ‰¾åˆ°ä¸ç‰¹å®šå®ä½“ç›¸å…³çš„æ‰€æœ‰çŸ¥è¯†èŠ‚ç‚¹

---

## 4. çŸ¥è¯†å›¾è°±çš„æ„å»ºæµç¨‹

å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºéœ€è¦ç»è¿‡ä¸‰ä¸ªä¸»è¦é˜¶æ®µï¼š

```
åŸå§‹æ–‡æ¡£
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é˜¶æ®µ 1: æ–‡æ¡£åˆ‡åˆ† (Chunking)            â”‚
â”‚ å·¥å…·: pikerag/workflows/chunking.py    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
æ–‡æ¡£å— (Chunks)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é˜¶æ®µ 2: åŸå­é—®é¢˜æå– (Tagging)         â”‚
â”‚ å·¥å…·: pikerag/workflows/tagging.py     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
å¸¦åŸå­é—®é¢˜çš„æ–‡æ¡£å— (Tagged Chunks)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é˜¶æ®µ 3: å‘é‡æ•°æ®åº“æ„å»º                 â”‚
â”‚ å·¥å…·: ChunkAtomRetriever è‡ªåŠ¨åŠ è½½      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
å¼‚æ„çŸ¥è¯†å›¾è°± (Chunk Store + Atom Store)
```

### 4.1 é˜¶æ®µ 1: æ–‡æ¡£åˆ‡åˆ† (Chunking)

#### ç›®çš„
å°†å¤§å‹æ–‡æ¡£åˆ‡åˆ†æˆåˆé€‚å¤§å°çš„ Chunkï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§

#### æ‰§è¡Œå‘½ä»¤
```bash
python examples/chunking.py examples/biology/configs/chunking.yml
```

#### æ ¸å¿ƒä»£ç æµç¨‹

**æ–‡ä»¶**: `pikerag/workflows/chunking.py`

```python
class ChunkingWorkflow:
    def __init__(self, yaml_config: dict):
        # 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._init_llm_client()
        
        # 2. åˆå§‹åŒ–åˆ†å‰²å™¨
        self._init_splitter()  # é€šå¸¸ä½¿ç”¨ LLMPoweredRecursiveSplitter
        
    def run(self):
        for doc_name, input_path, output_path in self._file_infos:
            # 3. åŠ è½½åŸå§‹æ–‡æ¡£
            doc_loader = get_loader(file_path=input_path)
            docs = doc_loader.load()
            
            # 4. æ‰§è¡Œæ–‡æ¡£åˆ‡åˆ†
            chunk_docs = self._splitter.transform_documents(docs)
            
            # 5. ä¿å­˜åˆ‡åˆ†ç»“æœåˆ° pickle æ–‡ä»¶
            with open(output_path, "wb") as fout:
                pickle.dump(chunk_docs, fout)
```

#### è¾“å‡ºç¤ºä¾‹

**è¾“å‡ºæ–‡ä»¶**: `data/biology/chunks.pkl`

```python
[
    Document(
        page_content="ç»†èƒæ˜¯ç”Ÿç‰©ä½“ç»“æ„å’ŒåŠŸèƒ½çš„åŸºæœ¬å•ä½ã€‚çœŸæ ¸ç»†èƒåŒ…å«ç»†èƒæ ¸ã€
                     ç»†èƒè´¨å’Œç»†èƒè†œç­‰ç»“æ„ã€‚ç»†èƒæ ¸æ˜¯é—ä¼ ä¿¡æ¯çš„å‚¨å­˜ä¸­å¿ƒ...",
        metadata={
            "filename": "biology_textbook.pdf",
            "page": 15
        }
    ),
    Document(
        page_content="çº¿ç²’ä½“è¢«ç§°ä¸ºç»†èƒçš„èƒ½é‡å·¥å‚ï¼Œè´Ÿè´£è¿›è¡Œç»†èƒå‘¼å¸äº§ç”ŸATPã€‚
                     çº¿ç²’ä½“å…·æœ‰åŒå±‚è†œç»“æ„ï¼Œå¤–è†œå…‰æ»‘ï¼Œå†…è†œå‘å†…æŠ˜å å½¢æˆåµ´...",
        metadata={
            "filename": "biology_textbook.pdf",
            "page": 16
        }
    ),
    # ... æ›´å¤š chunks
]
```

#### é…ç½®ç¤ºä¾‹

**æ–‡ä»¶**: `examples/biology/configs/chunking.yml`

```yaml
splitter:
  module_path: pikerag.document_transformers.splitter
  class_name: LLMPoweredRecursiveSplitter
  args:
    chunk_size: 1000        # æ¯ä¸ª chunk çš„ç›®æ ‡å¤§å°
    chunk_overlap: 200      # chunk ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
    separators:             # åˆ†éš”ç¬¦ä¼˜å…ˆçº§
      - "\n\n"
      - "\n"
      - "ã€‚"
      - "ï¼"
      - "ï¼Ÿ"
```

---

### 4.2 é˜¶æ®µ 2: åŸå­é—®é¢˜æå– (Tagging)

#### ç›®çš„
ä½¿ç”¨ LLM ä»æ¯ä¸ª Chunk ä¸­æå–å¤šä¸ªå¯å›ç­”çš„åŸå­é—®é¢˜ï¼ˆAtomsï¼‰

#### æ‰§è¡Œå‘½ä»¤
```bash
python examples/tagging.py examples/hotpotqa/configs/tagging.yml
```

#### æ ¸å¿ƒä»£ç æµç¨‹

**æ–‡ä»¶**: `pikerag/workflows/tagging.py`

```python
class TaggingWorkflow:
    def __init__(self, yaml_config: dict):
        # 1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        self._init_llm_client()
        
        # 2. åˆå§‹åŒ– Tagger
        self._tagger = LLMPoweredTagger(
            llm_client=self._client,
            tagging_protocol=self._tagging_protocol,  # å®šä¹‰å¦‚ä½•æå–é—®é¢˜
            tag_name="atom_questions"
        )
        
    def run(self):
        # 3. åŠ è½½åˆ‡åˆ†åçš„æ–‡æ¡£
        docs = self._load_func(**self._load_args)  
        # ä¾‹å¦‚: load_chunks_from_jsonl()
        
        # 4. å¯¹æ¯ä¸ª Chunk æå–åŸå­é—®é¢˜
        tagged_docs = self._tagger.transform_documents(docs)
        
        # 5. ä¿å­˜å¸¦æ ‡ç­¾çš„æ–‡æ¡£
        self._save_func(tagged_docs, **self._save_args)
        # ä¾‹å¦‚: save_chunks_to_jsonl()
```

**æ–‡ä»¶**: `pikerag/document_transformers/tagger/llm_powered_tagger.py`

```python
class LLMPoweredTagger:
    def _get_tags_info(self, content: str, **metadata) -> List[str]:
        # 1. ä½¿ç”¨åè®®æ„å»º Prompt
        messages = self._tagging_protocol.process_input(
            content=content, 
            **metadata
        )
        
        # 2. è°ƒç”¨ LLM ç”ŸæˆåŸå­é—®é¢˜
        response = self._llm_client.generate_content_with_messages(
            messages=messages,
            **self._llm_config
        )
        
        # 3. è§£æ LLM è¾“å‡ºä¸ºé—®é¢˜åˆ—è¡¨
        return self._tagging_protocol.parse_output(
            content=response, 
            **metadata
        )
```

#### Prompt æ¨¡æ¿

**æ–‡ä»¶**: `pikerag/prompts/tagging/atom_question_tagging.py`

```python
atom_question_tagging_template = MessageTemplate(
    template=[
        ("system", "You are a helpful AI assistant good at content "
                   "understanding and asking question."),
        ("user", """
# Task
Your task is to extract as many questions as possible that are 
relevant and can be answered by the given content. Please try to 
be diverse and avoid extracting duplicated or similar questions. 
Make sure your question contain necessary entity names and avoid 
to use pronouns like it, he, she, they, the company, the person etc.

# Output Format
Output your answers line by line, with each question on a new line, 
without itemized symbols or numbers.

# Content
{content}

# Output:
""".strip()),
    ]
)
```

#### LLM äº¤äº’ç¤ºä¾‹

**è¾“å…¥åˆ° LLM**:
```
Content: ã€Šå¯„ç”Ÿè™«ã€‹(Parasite)æ˜¯ç”±éŸ©å›½å¯¼æ¼”å¥‰ä¿Šæ˜Šæ‰§å¯¼çš„2019å¹´é»‘è‰²å–œå‰§æƒŠæ‚šç‰‡ã€‚
è¯¥ç‰‡åœ¨2020å¹´ç¬¬92å±Šå¥¥æ–¯å¡é‡‘åƒå¥–ä¸Šè·å¾—æœ€ä½³å½±ç‰‡ã€æœ€ä½³å¯¼æ¼”ã€æœ€ä½³åŸåˆ›å‰§æœ¬å’Œ
æœ€ä½³å›½é™…å½±ç‰‡å››é¡¹å¤§å¥–ã€‚å¥‰ä¿Šæ˜Šå¯¼æ¼”1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚ã€‚
```

**LLM è¾“å‡º**:
```
ã€Šå¯„ç”Ÿè™«ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ
ã€Šå¯„ç”Ÿè™«ã€‹æ˜¯å“ªä¸€å¹´çš„ç”µå½±ï¼Ÿ
ã€Šå¯„ç”Ÿè™«ã€‹åœ¨2020å¹´å¥¥æ–¯å¡è·å¾—äº†å“ªäº›å¥–é¡¹ï¼Ÿ
ã€Šå¯„ç”Ÿè™«ã€‹è·å¾—äº†å¤šå°‘ä¸ªå¥¥æ–¯å¡å¥–é¡¹ï¼Ÿ
å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ
å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªä¸€å¹´ï¼Ÿ
2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡æ˜¯å“ªéƒ¨ç”µå½±ï¼Ÿ
2020å¹´å¥¥æ–¯å¡æœ€ä½³å¯¼æ¼”æ˜¯è°ï¼Ÿ
```

#### è¾“å‡ºç¤ºä¾‹

**è¾“å‡ºæ–‡ä»¶**: `data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl`

```json
{
  "chunk_id": "chunk_001",
  "title": "2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡",
  "content": "ã€Šå¯„ç”Ÿè™«ã€‹(Parasite)æ˜¯ç”±éŸ©å›½å¯¼æ¼”å¥‰ä¿Šæ˜Šæ‰§å¯¼çš„2019å¹´é»‘è‰²å–œå‰§æƒŠæ‚šç‰‡...",
  "atom_questions": [
    "ã€Šå¯„ç”Ÿè™«ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ",
    "ã€Šå¯„ç”Ÿè™«ã€‹æ˜¯å“ªä¸€å¹´çš„ç”µå½±ï¼Ÿ",
    "ã€Šå¯„ç”Ÿè™«ã€‹åœ¨2020å¹´å¥¥æ–¯å¡è·å¾—äº†å“ªäº›å¥–é¡¹ï¼Ÿ",
    "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
    "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªä¸€å¹´ï¼Ÿ"
  ]
}
```

#### é…ç½®ç¤ºä¾‹

**æ–‡ä»¶**: `examples/hotpotqa/configs/tagging.yml`

```yaml
# è¾“å…¥æ–‡æ¡£åŠ è½½
ori_doc_loading:
  module: pikerag.utils.data_protocol_utils
  name: load_chunks_from_jsonl
  args:
    jsonl_chunk_path: data/hotpotqa/dev_500_retrieval_contexts_as_chunks.jsonl

# è¾“å‡ºæ–‡æ¡£ä¿å­˜
tagged_doc_saving:
  module: pikerag.utils.data_protocol_utils
  name: save_chunks_to_jsonl
  args:
    dump_path: data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl

# Tagger è®¾ç½®
tagger:
  tagging_protocol:
    module_path: pikerag.prompts.tagging
    attr_name: atom_question_tagging_protocol
  tag_name: atom_questions

# LLM è®¾ç½®
llm_client:
  module_path: pikerag.llm_client
  class_name: AzureOpenAIClient
  llm_config:
    model: gpt-4
    temperature: 0.7
```

---

### 4.3 é˜¶æ®µ 3: å‘é‡æ•°æ®åº“æ„å»º

#### ç›®çš„
è‡ªåŠ¨åŠ è½½ Tagged Chunksï¼Œæ„å»ºåŒå‘é‡å­˜å‚¨ï¼ˆChunk Store + Atom Storeï¼‰

#### ç‰¹ç‚¹
è¿™ä¸ªé˜¶æ®µ**ä¸éœ€è¦å•ç‹¬æ‰§è¡Œ**ï¼Œåœ¨ QA å·¥ä½œæµåˆå§‹åŒ–æ—¶è‡ªåŠ¨å®Œæˆ

#### æ ¸å¿ƒä»£ç æµç¨‹

**æ–‡ä»¶**: `pikerag/knowledge_retrievers/chunk_atom_retriever.py`

```python
class ChunkAtomRetriever:
    def _load_vector_store(self):
        # 1. åŠ è½½å¸¦åŸå­é—®é¢˜çš„ Chunks
        doc_ids, docs = load_ids_and_chunks(
            filepath="data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl",
            atom_tag="atom_questions"
        )
        
        # 2. æ„å»º Chunk å‘é‡å­˜å‚¨
        self._chunk_store: Chroma = load_vector_store(
            collection_name="hotpotqa_chunk",
            persist_directory="data/vector_stores/hotpotqa",
            embedding=self.embedding_func,
            documents=docs,         # List[Document]
            ids=doc_ids,           # List[str]
            exist_ok=True
        )
        
        # 3. åŠ è½½ Atomsï¼ˆä»åŒä¸€ä¸ª JSONL æ–‡ä»¶ï¼‰
        atom_ids, atoms = load_ids_and_atoms(
            filepath="data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl",
            atom_tag="atom_questions"
        )
        
        # 4. æ„å»º Atom å‘é‡å­˜å‚¨
        self._atom_store: Chroma = load_vector_store(
            collection_name="hotpotqa_atom",
            persist_directory="data/vector_stores/hotpotqa",
            embedding=self.embedding_func,
            documents=atoms,        # List[Document]
            ids=atom_ids,          # None (è‡ªåŠ¨ç”Ÿæˆ)
            exist_ok=True
        )
```

#### æ•°æ®åŠ è½½å‡½æ•°è¯¦è§£

**æ–‡ä»¶**: `pikerag/utils/data_protocol_utils.py`

**å‡½æ•° 1: load_ids_and_chunks**

```python
def load_ids_and_chunks(
    filepath: str, 
    atom_tag: str = "atom_questions"
) -> Tuple[List[str], List[Document]]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½ Chunksï¼Œæ„å»º Chunk Store
    
    Returns:
        chunk_ids: ['chunk_001', 'chunk_002', ...]
        chunk_docs: [Document(...), Document(...), ...]
    """
    chunk_ids: List[str] = []
    chunk_docs: List[Document] = []
    
    with jsonlines.open(filepath, "r") as reader:
        for chunk_dict in reader:
            chunk_ids.append(chunk_dict["chunk_id"])
            
            chunk_docs.append(
                Document(
                    page_content=chunk_dict["content"],
                    metadata={
                        "id": chunk_dict["chunk_id"],
                        "title": chunk_dict["title"],
                        # å°† atom_questions åˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²ä¿å­˜
                        "atom_questions_str": "\n".join(chunk_dict[atom_tag])
                    }
                )
            )
    
    return chunk_ids, chunk_docs
```

**å‡½æ•° 2: load_ids_and_atoms**

```python
def load_ids_and_atoms(
    filepath: str, 
    atom_tag: str
) -> Tuple[None, List[Document]]:
    """
    ä» JSONL æ–‡ä»¶åŠ è½½ Atomsï¼Œæ„å»º Atom Store
    
    Returns:
        None: atom_ids è‡ªåŠ¨ç”Ÿæˆ
        atom_docs: [Document(...), Document(...), ...]
    """
    atom_docs: List[Document] = []
    
    with jsonlines.open(filepath, "r") as reader:
        for chunk_dict in reader:
            # éå†æ¯ä¸ª chunk çš„æ‰€æœ‰åŸå­é—®é¢˜
            for atom in chunk_dict[atom_tag]:
                atom = atom.strip()
                if len(atom) > 0:
                    atom_docs.append(
                        Document(
                            page_content=atom,  # å•ä¸ªåŸå­é—®é¢˜
                            metadata={
                                # å…³é”®: è®°å½•æº Chunk ID
                                "source_chunk_id": chunk_dict["chunk_id"],
                                "title": chunk_dict["title"]
                            }
                        )
                    )
    
    return None, atom_docs
```

#### å‘é‡åŒ–ä¸å­˜å‚¨

```python
# Chroma ä¼šè‡ªåŠ¨å¯¹æ¯ä¸ª Document çš„ page_content è¿›è¡Œå‘é‡åŒ–
embedding_func = AzureOpenAIEmbedding()  # ä¾‹å¦‚ä½¿ç”¨ text-embedding-ada-002

# Chunk Store ä¸­å­˜å‚¨çš„å‘é‡
chunk_vector = embedding_func.embed_query(chunk.page_content)
# å‘é‡ç»´åº¦: ä¾‹å¦‚ 1536 (ada-002)

# Atom Store ä¸­å­˜å‚¨çš„å‘é‡
atom_vector = embedding_func.embed_query(atom.page_content)
# å‘é‡ç»´åº¦: ä¾‹å¦‚ 1536 (ada-002)
```

#### æœ€ç»ˆæ•°æ®åº“ç»“æ„

```
data/vector_stores/hotpotqa/
â”œâ”€â”€ chroma.sqlite3                          # Chroma æ•°æ®åº“æ–‡ä»¶
â””â”€â”€ collections/
    â”œâ”€â”€ hotpotqa_chunk/                     # Chunk é›†åˆ
    â”‚   â”œâ”€â”€ vectors.bin                     # Chunk å‘é‡
    â”‚   â””â”€â”€ metadata.json                   # Chunk å…ƒæ•°æ®
    â””â”€â”€ hotpotqa_atom/                      # Atom é›†åˆ
        â”œâ”€â”€ vectors.bin                     # Atom å‘é‡
        â””â”€â”€ metadata.json                   # Atom å…ƒæ•°æ®
```

#### æ•°æ®ç»Ÿè®¡ç¤ºä¾‹

å‡è®¾å¤„ç† HotpotQA dev_500 æ•°æ®é›†ï¼š

```
åŸå§‹é—®é¢˜æ•°: 500
æ£€ç´¢ä¸Šä¸‹æ–‡æ®µè½æ•°: 5,000
â†“ [Chunking]
Chunks æ•°é‡: 5,000
â†“ [Tagging]
Atoms æ€»æ•°: 25,000 (å¹³å‡æ¯ä¸ª Chunk 5 ä¸ª Atom)
â†“ [Vector Store]
Chunk Store: 5,000 ä¸ªå‘é‡
Atom Store: 25,000 ä¸ªå‘é‡
```

---

## 5. çŸ¥è¯†å›¾è°±çš„æ£€ç´¢æœºåˆ¶

ChunkAtomRetriever æä¾›äº†ä¸‰ç§æ£€ç´¢æ–¹æ³•ï¼Œå¯¹åº”ä¸åŒçš„æ£€ç´¢ç­–ç•¥ï¼š

```
ç”¨æˆ·é—®é¢˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ChunkAtomRetriever æ£€ç´¢æ–¹æ³•             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ–¹æ³• 1: retrieve_atom_info_through_atom()       â”‚
â”‚   â†’ é€šè¿‡ Atom Store æ£€ç´¢                        â”‚
â”‚                                                  â”‚
â”‚ æ–¹æ³• 2: retrieve_atom_info_through_chunk()      â”‚
â”‚   â†’ é€šè¿‡ Chunk Store æ£€ç´¢ï¼Œè¿”å›æœ€ä½³ Atom        â”‚
â”‚                                                  â”‚
â”‚ æ–¹æ³• 3: retrieve_contents_by_query()            â”‚
â”‚   â†’ ç»¼åˆæ£€ç´¢ï¼ˆæ–¹æ³• 1 + æ–¹æ³• 2ï¼‰                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¿”å›ç›¸å…³ Chunks
```

### 5.1 æ–¹æ³• 1: retrieve_atom_info_through_atom()

#### åŸç†
ç›´æ¥åœ¨ Atom Store ä¸­è¿›è¡Œå‘é‡æ£€ç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„åŸå­é—®é¢˜ï¼Œç„¶åé€šè¿‡ `source_chunk_id` è·å–æº Chunk

#### æµç¨‹å›¾

```
ç”¨æˆ·é—®é¢˜: "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
    â†“
ã€æ­¥éª¤ 1ã€‘å‘é‡åŒ–é—®é¢˜
    query_embedding = embed("å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ")
    â†“
ã€æ­¥éª¤ 2ã€‘åœ¨ Atom Store ä¸­æ£€ç´¢ Top-K æœ€ç›¸ä¼¼çš„ Atoms
    â†“
    Atom 1: "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ" (score: 0.95, source_chunk_id: chunk_001)
    Atom 2: "å¥‰ä¿Šæ˜Šå¯¼æ¼”çš„å›½ç±æ˜¯ä»€ä¹ˆï¼Ÿ" (score: 0.82, source_chunk_id: chunk_001)
    Atom 3: "å¥‰ä¿Šæ˜Šå¯¼æ¼”çš„æˆé•¿ç»å†ï¼Ÿ" (score: 0.78, source_chunk_id: chunk_005)
    â†“
ã€æ­¥éª¤ 3ã€‘æå–æ‰€æœ‰å”¯ä¸€çš„ source_chunk_id
    unique_chunk_ids = ["chunk_001", "chunk_005"]
    â†“
ã€æ­¥éª¤ 4ã€‘ä» Chunk Store æ‰¹é‡è·å– Chunks
    chunks = _chunk_store.get(ids=unique_chunk_ids)
    â†“
ã€æ­¥éª¤ 5ã€‘ç»„è£… AtomRetrievalInfo åˆ—è¡¨
    [
        AtomRetrievalInfo(
            atom_query="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
            atom="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
            source_chunk_id="chunk_001",
            source_chunk="ã€Šå¯„ç”Ÿè™«ã€‹...å¥‰ä¿Šæ˜Š1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚...",
            retrieval_score=0.95
        ),
        ...
    ]
```

#### ä»£ç å®ç°

**æ–‡ä»¶**: `pikerag/knowledge_retrievers/chunk_atom_retriever.py`

```python
def retrieve_atom_info_through_atom(
    self, 
    queries: Union[List[str], str], 
    retrieve_id: str = "",
    **kwargs
) -> List[AtomRetrievalInfo]:
    """
    é€šè¿‡ Atom Store æ£€ç´¢
    
    Args:
        queries: å•ä¸ªæˆ–å¤šä¸ªæŸ¥è¯¢é—®é¢˜
        retrieve_id: æ£€ç´¢æ ‡è¯†ç¬¦ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        **kwargs: å¯é€‰å‚æ•°ï¼Œå¦‚ retrieve_k
    
    Returns:
        List[AtomRetrievalInfo]: æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    # 1. å†³å®š retrieve_kï¼ˆæ¯ä¸ªæŸ¥è¯¢è¿”å›å¤šå°‘ä¸ªç»“æœï¼‰
    if "retrieve_k" in kwargs:
        retrieve_k = kwargs["retrieve_k"]
    elif isinstance(queries, list) and len(queries) > 1:
        retrieve_k = self.atom_retrieve_k  # å¤šæŸ¥è¯¢æ—¶ç”¨è¾ƒå°çš„ k
    else:
        retrieve_k = self.retrieve_k       # å•æŸ¥è¯¢æ—¶ç”¨æ ‡å‡† k
    
    # 2. ç¡®ä¿ queries æ˜¯åˆ—è¡¨
    if isinstance(queries, str):
        queries = [queries]
    
    # 3. å¯¹æ¯ä¸ª query åœ¨ Atom Store ä¸­æ£€ç´¢
    query_atom_score_tuples: List[Tuple[str, Document, float]] = []
    for atom_query in queries:
        for atom_doc, score in self._get_doc_with_query(
            atom_query, 
            self._atom_store,  # åœ¨ Atom Store ä¸­æ£€ç´¢
            retrieve_k
        ):
            query_atom_score_tuples.append((atom_query, atom_doc, score))
    
    # 4. è½¬æ¢ä¸º AtomRetrievalInfo å¯¹è±¡
    return self._atom_info_tuple_to_class(query_atom_score_tuples)


def _atom_info_tuple_to_class(
    self, 
    atom_retrieval_info: List[Tuple[str, Document, float]]
) -> List[AtomRetrievalInfo]:
    """
    å°†æ£€ç´¢ç»“æœè½¬æ¢ä¸º AtomRetrievalInfo å¯¹è±¡
    
    æ ¸å¿ƒé€»è¾‘:
    1. æå–æ‰€æœ‰å”¯ä¸€çš„ source_chunk_id
    2. æ‰¹é‡ä» Chunk Store è·å– Chunks
    3. ç»„è£…å®Œæ•´çš„æ£€ç´¢ä¿¡æ¯
    """
    # 1. æå–å”¯ä¸€çš„ source_chunk_id
    source_chunk_ids: List[str] = list(set([
        doc.metadata["source_chunk_id"] 
        for _, doc, _ in atom_retrieval_info
    ]))
    
    # 2. æ‰¹é‡è·å– Chunksï¼ˆå…³é”®: å›¾è°±çš„è¾¹éå†ï¼‰
    chunk_doc_results: Dict[str, Any] = self._chunk_store.get(
        ids=source_chunk_ids
    )
    
    # 3. æ„å»º chunk_id -> chunk_content æ˜ å°„
    chunk_id_to_content = {
        chunk_id: chunk_str
        for chunk_id, chunk_str in zip(
            chunk_doc_results["ids"], 
            chunk_doc_results["documents"]
        )
    }
    
    # 4. ç»„è£… AtomRetrievalInfo
    retrieval_infos: List[AtomRetrievalInfo] = []
    for atom_query, atom_doc, score in atom_retrieval_info:
        source_chunk_id = atom_doc.metadata["source_chunk_id"]
        retrieval_infos.append(
            AtomRetrievalInfo(
                atom_query=atom_query,
                atom=atom_doc.page_content,
                source_chunk_title=atom_doc.metadata.get("title", None),
                source_chunk=chunk_id_to_content[source_chunk_id],  # è·å–å®Œæ•´ Chunk
                source_chunk_id=source_chunk_id,
                retrieval_score=score,
                atom_embedding=self.embedding_func.embed_query(atom_doc.page_content)
            )
        )
    
    return retrieval_infos
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
retriever = ChunkAtomRetriever(...)

# å•æŸ¥è¯¢æ£€ç´¢
results = retriever.retrieve_atom_info_through_atom(
    queries="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
    retrieve_id="Q001"
)

# å¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆç”¨äºé—®é¢˜åˆ†è§£åœºæ™¯ï¼‰
results = retriever.retrieve_atom_info_through_atom(
    queries=[
        "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
        "2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡æ˜¯ä»€ä¹ˆï¼Ÿ",
        "ã€Šå¯„ç”Ÿè™«ã€‹è·å¾—äº†å“ªäº›å¥–é¡¹ï¼Ÿ"
    ],
    retrieve_id="Q001_decomposed"
)

# è®¿é—®ç»“æœ
for info in results:
    print(f"Query: {info.atom_query}")
    print(f"Matched Atom: {info.atom}")
    print(f"Score: {info.retrieval_score}")
    print(f"Source Chunk: {info.source_chunk[:100]}...")
```

---

### 5.2 æ–¹æ³• 2: retrieve_atom_info_through_chunk()

#### åŸç†
åœ¨ Chunk Store ä¸­è¿›è¡Œå‘é‡æ£€ç´¢ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„ Chunksï¼Œç„¶åä¸ºæ¯ä¸ª Chunk æ‰¾åˆ°ä¸é—®é¢˜æœ€åŒ¹é…çš„ä¸€ä¸ª Atom

#### æµç¨‹å›¾

```
ç”¨æˆ·é—®é¢˜: "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
    â†“
ã€æ­¥éª¤ 1ã€‘å‘é‡åŒ–é—®é¢˜
    query_embedding = embed("å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ")
    â†“
ã€æ­¥éª¤ 2ã€‘åœ¨ Chunk Store ä¸­æ£€ç´¢ Top-K æœ€ç›¸ä¼¼çš„ Chunks
    â†“
    Chunk 1: "ã€Šå¯„ç”Ÿè™«ã€‹...å¥‰ä¿Šæ˜Š1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚..." (score: 0.88)
      metadata: {
        "atom_questions_str": "ã€Šå¯„ç”Ÿè™«ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ\nå¥‰ä¿Šæ˜Šå‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ\n..."
      }
    Chunk 2: "å¥‰ä¿Šæ˜Šçš„æ—©æœŸä½œå“..." (score: 0.75)
      metadata: {
        "atom_questions_str": "å¥‰ä¿Šæ˜Šçš„ç¬¬ä¸€éƒ¨ç”µå½±ï¼Ÿ\nå¥‰ä¿Šæ˜Šçš„ä»£è¡¨ä½œï¼Ÿ\n..."
      }
    â†“
ã€æ­¥éª¤ 3ã€‘å¯¹æ¯ä¸ª Chunkï¼Œè®¡ç®—é—®é¢˜ä¸å…¶æ‰€æœ‰ Atoms çš„ç›¸ä¼¼åº¦
    Chunk 1 çš„ Atoms:
      - "ã€Šå¯„ç”Ÿè™«ã€‹çš„å¯¼æ¼”æ˜¯è°ï¼Ÿ" â†’ ç›¸ä¼¼åº¦: 0.72
      - "å¥‰ä¿Šæ˜Šå‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ" â†’ ç›¸ä¼¼åº¦: 0.95 âœ“ (æœ€é«˜)
      - ...
    
    Chunk 2 çš„ Atoms:
      - "å¥‰ä¿Šæ˜Šçš„ç¬¬ä¸€éƒ¨ç”µå½±ï¼Ÿ" â†’ ç›¸ä¼¼åº¦: 0.68 âœ“ (æœ€é«˜)
      - ...
    â†“
ã€æ­¥éª¤ 4ã€‘ä¸ºæ¯ä¸ª Chunk é€‰æ‹©æœ€ä½³ Atom
    [
        (Chunk 1, "å¥‰ä¿Šæ˜Šå‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ", 0.95),
        (Chunk 2, "å¥‰ä¿Šæ˜Šçš„ç¬¬ä¸€éƒ¨ç”µå½±ï¼Ÿ", 0.68)
    ]
    â†“
ã€æ­¥éª¤ 5ã€‘ç»„è£… AtomRetrievalInfo åˆ—è¡¨
```

#### ä»£ç å®ç°

```python
def retrieve_atom_info_through_chunk(
    self, 
    query: str, 
    retrieve_id: str = ""
) -> List[AtomRetrievalInfo]:
    """
    é€šè¿‡ Chunk Store æ£€ç´¢ï¼Œè¿”å›æ¯ä¸ª Chunk çš„æœ€ä½³åŒ¹é… Atom
    
    Args:
        query: æŸ¥è¯¢é—®é¢˜
        retrieve_id: æ£€ç´¢æ ‡è¯†ç¬¦
    
    Returns:
        List[AtomRetrievalInfo]: æ£€ç´¢ç»“æœåˆ—è¡¨
    """
    # 1. åœ¨ Chunk Store ä¸­æ£€ç´¢
    chunk_info: List[Tuple[Document, float]] = self._get_doc_with_query(
        query, 
        self._chunk_store,  # åœ¨ Chunk Store ä¸­æ£€ç´¢
        self.retrieve_k
    )
    
    # 2. ä¸ºæ¯ä¸ª Chunk æ‰¾åˆ°æœ€ä½³ Atom
    return self._chunk_info_tuple_to_class(
        query=query, 
        chunk_docs=[doc for doc, _ in chunk_info]
    )


def _chunk_info_tuple_to_class(
    self, 
    query: str, 
    chunk_docs: List[Document]
) -> List[AtomRetrievalInfo]:
    """
    ä¸ºæ¯ä¸ª Chunk è®¡ç®—æœ€ä½³åŒ¹é…çš„ Atom
    """
    # 1. å‘é‡åŒ–ç”¨æˆ·é—®é¢˜
    query_embedding = self.embedding_func.embed_query(query)
    
    # 2. ä¸ºæ¯ä¸ª Chunk æ‰¾åˆ°æœ€ä½³ Atom
    best_hit_atom_infos: List[Tuple[str, float, List[float]]] = []
    
    for chunk_doc in chunk_docs:
        best_atom, best_score, best_embedding = "", 0, []
        
        # éå†è¯¥ Chunk çš„æ‰€æœ‰ Atoms (å­˜å‚¨åœ¨ metadata ä¸­)
        for atom in chunk_doc.metadata["atom_questions_str"].split("\n"):
            # å‘é‡åŒ– Atom
            atom_embedding = self.embedding_func.embed_query(atom)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            score = self.similarity_func(query_embedding, atom_embedding)
            
            # æ›´æ–°æœ€ä½³åŒ¹é…
            if score > best_score:
                best_atom = atom
                best_score = score
                best_embedding = atom_embedding
        
        best_hit_atom_infos.append((best_atom, best_score, best_embedding))
    
    # 3. ç»„è£… AtomRetrievalInfo
    retrieval_infos: List[AtomRetrievalInfo] = []
    for chunk_doc, (atom, score, atom_embedding) in zip(chunk_docs, best_hit_atom_infos):
        retrieval_infos.append(
            AtomRetrievalInfo(
                atom_query=query,
                atom=atom,
                source_chunk_title=chunk_doc.metadata.get("title", None),
                source_chunk=chunk_doc.page_content,
                source_chunk_id=chunk_doc.metadata["id"],
                retrieval_score=score,
                atom_embedding=atom_embedding
            )
        )
    
    return retrieval_infos
```

#### æ–¹æ³• 1 vs æ–¹æ³• 2 å¯¹æ¯”

| ç»´åº¦ | æ–¹æ³• 1: through_atom | æ–¹æ³• 2: through_chunk |
|------|---------------------|---------------------|
| **æ£€ç´¢å¯¹è±¡** | Atom Store | Chunk Store |
| **æ£€ç´¢ç²’åº¦** | ç»†ç²’åº¦ï¼ˆåŸå­é—®é¢˜ï¼‰ | ç²—ç²’åº¦ï¼ˆæ–‡æ¡£å—ï¼‰ |
| **ç²¾ç¡®åº¦** | é«˜ï¼ˆç›´æ¥åŒ¹é… Atomï¼‰ | ä¸­ï¼ˆéœ€äºŒæ¬¡è®¡ç®—ï¼‰ |
| **è®¡ç®—æˆæœ¬** | ä½ï¼ˆä¸€æ¬¡å‘é‡æ£€ç´¢ï¼‰ | é«˜ï¼ˆæ£€ç´¢ + N æ¬¡ç›¸ä¼¼åº¦è®¡ç®—ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | ç²¾ç¡®é—®ç­”ã€é—®é¢˜åˆ†è§£ | æ¢ç´¢æ€§æ£€ç´¢ã€ä¸Šä¸‹æ–‡ä¸°å¯Œ |

---

### 5.3 æ–¹æ³• 3: retrieve_contents_by_query() - ç»¼åˆæ£€ç´¢

#### åŸç†
ç»“åˆæ–¹æ³• 1 å’Œæ–¹æ³• 2ï¼Œæ—¢åœ¨ Atom Store æ£€ç´¢ï¼Œä¹Ÿåœ¨ Chunk Store æ£€ç´¢ï¼Œå»é‡åè¿”å›

#### æµç¨‹å›¾

```
ç”¨æˆ·é—®é¢˜
    â†“
ã€è·¯å¾„ Aã€‘é€šè¿‡ Atom Store æ£€ç´¢
    â†“
    Atom 1 â†’ Chunk A
    Atom 2 â†’ Chunk A (å»é‡)
    Atom 3 â†’ Chunk B
    â†“
ã€è·¯å¾„ Bã€‘é€šè¿‡ Chunk Store æ£€ç´¢
    â†“
    Chunk C
    Chunk A (å»é‡)
    â†“
ã€åˆå¹¶å»é‡ã€‘
    â†“
è¿”å›: [Chunk A, Chunk B, Chunk C]
```

#### ä»£ç å®ç°

```python
def retrieve_contents_by_query(
    self, 
    query: str, 
    retrieve_id: str = ""
) -> List[str]:
    """
    ç»¼åˆæ£€ç´¢: åŒæ—¶ä½¿ç”¨ Atom Store å’Œ Chunk Store
    
    Returns:
        List[str]: å»é‡åçš„ Chunk å†…å®¹åˆ—è¡¨
    """
    # 1. ä» Chunk Store ç›´æ¥æ£€ç´¢
    chunk_info: List[Tuple[Document, float]] = self._get_doc_with_query(
        query, 
        self._chunk_store, 
        self.retrieve_k
    )
    chunks = [chunk_doc.page_content for chunk_doc, _ in chunk_info]
    
    # 2. ä» Atom Store æ£€ç´¢ï¼Œè·å–æº Chunks
    atom_infos = self.retrieve_atom_info_through_atom(
        queries=query, 
        retrieve_id=retrieve_id
    )
    atom_source_chunks = [atom_info.source_chunk for atom_info in atom_infos]
    
    # 3. åˆå¹¶å¹¶å»é‡
    for chunk in atom_source_chunks:
        if chunk not in chunks:
            chunks.append(chunk)
    
    return chunks
```

#### ä½¿ç”¨åœºæ™¯

**åŸºç¡€ QA å·¥ä½œæµ**:

```python
# pikerag/workflows/qa.py
class QaWorkflow:
    def answer(self, qa: BaseQaData, question_idx: int) -> dict:
        # ä½¿ç”¨ç»¼åˆæ£€ç´¢
        reference_chunks = self._retriever.retrieve_contents(
            qa, 
            retrieve_id=f"Q{question_idx:03}"
        )
        
        # å°†æ£€ç´¢ç»“æœæä¾›ç»™ LLM
        messages = self._qa_protocol.process_input(
            content=qa.question, 
            references=reference_chunks
        )
        
        response = self._client.generate_content_with_messages(messages)
        return self._qa_protocol.parse_output(response)
```

---

## 6. æ•°æ®æµè½¬è¯¦è§£

### 6.1 å®Œæ•´æ•°æ®æµè½¬å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ç¦»çº¿é˜¶æ®µ: çŸ¥è¯†å›¾è°±æ„å»º                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åŸå§‹æ–‡æ¡£ (documents/)
    â†“
    | python examples/chunking.py config.yml
    â†“
æ–‡æ¡£å— (chunks.jsonl)
[
  {"chunk_id": "chunk_001", "content": "...", "title": "..."},
  {"chunk_id": "chunk_002", "content": "...", "title": "..."}
]
    â†“
    | python examples/tagging.py config.yml
    | (è°ƒç”¨ LLM æå–åŸå­é—®é¢˜)
    â†“
å¸¦æ ‡ç­¾æ–‡æ¡£å— (chunks_with_atoms.jsonl)
[
  {
    "chunk_id": "chunk_001",
    "content": "...",
    "title": "...",
    "atom_questions": ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
  }
]
    â†“
    | å‘é‡åŒ– + å­˜å‚¨
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           å¼‚æ„çŸ¥è¯†å›¾è°± (åŒå‘é‡å­˜å‚¨)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chunk Store                | Atom Store         â”‚
â”‚ - 5,000 chunks             | - 25,000 atoms     â”‚
â”‚ - å‘é‡åŒ–çš„å®Œæ•´æ–‡æ¡£å†…å®¹      | - å‘é‡åŒ–çš„åŸå­é—®é¢˜  â”‚
â”‚ - metadata: atom_questions | - metadata:        â”‚
â”‚                            |   source_chunk_id  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     åœ¨çº¿é˜¶æ®µ: é—®ç­”æ£€ç´¢                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç”¨æˆ·é—®é¢˜: "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
    â†“
    | QaWorkflow.answer()
    â†“
æ£€ç´¢å™¨åˆå§‹åŒ–
    â†“
    | ChunkAtomRetriever.__init__()
    | - è‡ªåŠ¨åŠ è½½ Chunk Store
    | - è‡ªåŠ¨åŠ è½½ Atom Store
    â†“
æ‰§è¡Œæ£€ç´¢
    â†“
    | retriever.retrieve_contents_by_query(query)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è·¯å¾„ A:        â”‚          â”‚  è·¯å¾„ B:        â”‚
â”‚  Atom æ£€ç´¢      â”‚          â”‚  Chunk æ£€ç´¢     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. å‘é‡åŒ–é—®é¢˜   â”‚          â”‚ 1. å‘é‡åŒ–é—®é¢˜   â”‚
â”‚ 2. æ£€ç´¢ Atoms   â”‚          â”‚ 2. æ£€ç´¢ Chunks  â”‚
â”‚ 3. è·å–æº Chunksâ”‚          â”‚ 3. ç›´æ¥è¿”å›     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
                åˆå¹¶ + å»é‡
                      â†“
            è¿”å› Top-K Chunks
            [Chunk A, Chunk B, Chunk C]
                      â†“
            ç»„è£… Prompt
                      â†“
        messages = [
          {"role": "system", "content": "..."},
          {"role": "user", "content": 
            "å‚è€ƒèµ„æ–™:\n{Chunk A}\n{Chunk B}\n\né—®é¢˜: å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
          }
        ]
                      â†“
            è°ƒç”¨ LLM
                      â†“
        response = client.generate(messages)
                      â†“
            è§£æè¾“å‡º
                      â†“
        answer = "å¥‰ä¿Šæ˜Šå¯¼æ¼”1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚"
```

### 6.2 æ•°æ®ç»“æ„è½¬æ¢é“¾

```
ã€é˜¶æ®µ 1: æ–‡æ¡£åˆ‡åˆ†ã€‘
Document (LangChain)
â”œâ”€â”€ page_content: str      # å®Œæ•´æ–‡æ¡£å—å†…å®¹
â””â”€â”€ metadata: dict
    â”œâ”€â”€ filename: str
    â””â”€â”€ page: int
    â†“ (ä¿å­˜ä¸º pickle)
List[Document] â†’ chunks.pkl

ã€é˜¶æ®µ 2: åŸå­é—®é¢˜æå–ã€‘
è¾“å…¥: List[Document] (from chunks.pkl)
    â†“
LLM æå–
    â†“
è¾“å‡º: List[Document]
â”œâ”€â”€ page_content: str      # å®Œæ•´æ–‡æ¡£å—å†…å®¹ (ä¸å˜)
â””â”€â”€ metadata: dict
    â”œâ”€â”€ chunk_id: str      # æ–°å¢
    â”œâ”€â”€ title: str         # æ–°å¢
    â””â”€â”€ atom_questions: List[str]  # æ–°å¢: LLM æå–çš„é—®é¢˜åˆ—è¡¨
    â†“ (ä¿å­˜ä¸º JSONL)
[
  {
    "chunk_id": "chunk_001",
    "title": "...",
    "content": "...",
    "atom_questions": ["Q1", "Q2", "Q3"]
  }
] â†’ chunks_with_atoms.jsonl

ã€é˜¶æ®µ 3: å‘é‡æ•°æ®åº“æ„å»ºã€‘

è¾“å…¥: chunks_with_atoms.jsonl
    â†“
load_ids_and_chunks()
    â†“
Chunk Store æ•°æ®ç»“æ„:
List[Document]
â”œâ”€â”€ page_content: str      # å®Œæ•´æ–‡æ¡£å—å†…å®¹
â””â”€â”€ metadata: dict
    â”œâ”€â”€ id: str            # chunk_id
    â”œâ”€â”€ title: str
    â””â”€â”€ atom_questions_str: str  # "Q1\nQ2\nQ3" (è½¬ä¸ºå­—ç¬¦ä¸²)
    â†“
å‘é‡åŒ– + å­˜å‚¨åˆ° Chroma
    â†“

load_ids_and_atoms()
    â†“
Atom Store æ•°æ®ç»“æ„:
List[Document]  (å±•å¼€: æ¯ä¸ª atom ä¸€ä¸ª Document)
â”œâ”€â”€ page_content: str      # å•ä¸ªåŸå­é—®é¢˜
â””â”€â”€ metadata: dict
    â”œâ”€â”€ source_chunk_id: str  # æŒ‡å‘æº Chunk
    â””â”€â”€ title: str
    â†“
å‘é‡åŒ– + å­˜å‚¨åˆ° Chroma

ã€é˜¶æ®µ 4: æ£€ç´¢ã€‘

ç”¨æˆ·é—®é¢˜: "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ"
    â†“
retrieve_atom_info_through_atom()
    â†“
æ£€ç´¢ç»“æœ: List[Tuple[str, Document, float]]
[
  (
    "å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",  # atom_query
    Document(
      page_content="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
      metadata={"source_chunk_id": "chunk_001", "title": "..."}
    ),
    0.95  # score
  ),
  ...
]
    â†“
_atom_info_tuple_to_class()
    â†“
æœ€ç»ˆè¾“å‡º: List[AtomRetrievalInfo]
[
  AtomRetrievalInfo(
    atom_query="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
    atom="å¥‰ä¿Šæ˜Šå¯¼æ¼”å‡ºç”Ÿåœ¨å“ªé‡Œï¼Ÿ",
    source_chunk_id="chunk_001",
    source_chunk="ã€Šå¯„ç”Ÿè™«ã€‹...å¥‰ä¿Šæ˜Š1969å¹´å‡ºç”ŸäºéŸ©å›½å¤§é‚±å¸‚...",
    source_chunk_title="2020å¹´å¥¥æ–¯å¡æœ€ä½³å½±ç‰‡",
    retrieval_score=0.95,
    atom_embedding=[0.123, 0.456, ...]
  ),
  ...
]
```

### 6.3 å…³é”®å­—æ®µå…³è”å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å…³è”å…³ç³»                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

chunks_with_atoms.jsonl
    â”œâ”€â”€ chunk_001
    â”‚   â”œâ”€â”€ content: "..." â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   â”œâ”€â”€ title: "..."                â”‚
    â”‚   â””â”€â”€ atom_questions:             â”‚
    â”‚       â”œâ”€â”€ "é—®é¢˜1" â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
    â”‚       â”œâ”€â”€ "é—®é¢˜2" â”€â”€â”€â”€â”   â”‚       â”‚
    â”‚       â””â”€â”€ "é—®é¢˜3" â”€â”  â”‚   â”‚       â”‚
    â”‚                    â”‚  â”‚   â”‚       â”‚
    â”œâ”€â”€ chunk_002        â”‚  â”‚   â”‚       â”‚
    â”‚   â”œâ”€â”€ content: ... â”‚  â”‚   â”‚       â”‚
    â”‚   â””â”€â”€ ...          â”‚  â”‚   â”‚       â”‚
    â””â”€â”€ ...              â”‚  â”‚   â”‚       â”‚
                         â”‚  â”‚   â”‚       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚       â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚       â”‚
        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
        â†“  â†“  â†“                         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Atom Store    â”‚         â”‚ Chunk Store  â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ Atom Doc 1     â”‚         â”‚ Chunk Doc 1  â”‚
    â”‚ - content: Q1  â”‚â”€ link â”€â†’â”‚ - id: c_001  â”‚
    â”‚ - meta:        â”‚         â”‚ - content:   â”‚
    â”‚   source_chunk â”‚         â”‚   "..."      â”‚
    â”‚   _id: c_001   â”‚         â”‚ - meta:      â”‚
    â”‚                â”‚         â”‚   atom_...   â”‚
    â”‚ Atom Doc 2     â”‚         â”‚   _str       â”‚
    â”‚ - content: Q2  â”‚â”€ link â”€â†’â”‚              â”‚
    â”‚ - meta:        â”‚         â”‚              â”‚
    â”‚   source_chunk â”‚         â”‚              â”‚
    â”‚   _id: c_001   â”‚         â”‚              â”‚
    â”‚                â”‚         â”‚              â”‚
    â”‚ Atom Doc 3     â”‚         â”‚              â”‚
    â”‚ - content: Q3  â”‚â”€ link â”€â†’â”‚              â”‚
    â”‚ - meta:        â”‚         â”‚              â”‚
    â”‚   source_chunk â”‚         â”‚              â”‚
    â”‚   _id: c_001   â”‚         â”‚              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†‘                          â†‘
            â”‚                          â”‚
        æ£€ç´¢ Atoms               æ£€ç´¢ Chunks
            â”‚                          â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
            åˆå¹¶è¿”å› Chunks ç»™ LLM
```

---

## 7. ä»£ç å®ç°è§£æ

### 7.1 æ ¸å¿ƒç±»å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å·¥ä½œæµå±‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ QaWorkflow                                         â”‚
â”‚ â”œâ”€â”€ QaDecompositionWorkflow (é—®é¢˜åˆ†è§£)            â”‚
â”‚ â”œâ”€â”€ QaIRCoTWorkflow (è¿­ä»£æ£€ç´¢)                    â”‚
â”‚ â”œâ”€â”€ QaSelfAskWorkflow (è‡ªæˆ‘è¯¢é—®)                  â”‚
â”‚ â””â”€â”€ QaIterRetgenWorkflow (è¿­ä»£ç”Ÿæˆ)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ uses
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 æ£€ç´¢å™¨å±‚                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BaseQaRetriever                                    â”‚
â”‚ â”œâ”€â”€ ChunkAtomRetriever â˜… (æ ¸å¿ƒ)                   â”‚
â”‚ â”œâ”€â”€ QaChunkRetriever                               â”‚
â”‚ â””â”€â”€ BM25Retriever                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ uses
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æ··å…¥å±‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ChromaMixin (å‘é‡æ•°æ®åº“æ“ä½œ)                       â”‚
â”‚ NetworkxMixin (å›¾éå†æ“ä½œ)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ uses
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 å­˜å‚¨å±‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Chroma (å‘é‡æ•°æ®åº“)                                â”‚
â”‚ NetworkX (å›¾æ•°æ®åº“)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 ChunkAtomRetriever å®Œæ•´å®ç°

**æ–‡ä»¶**: `pikerag/knowledge_retrievers/chunk_atom_retriever.py`

```python
from dataclasses import dataclass
from typing import List, Tuple, Union
import numpy as np
from langchain_chroma import Chroma
from langchain_core.documents import Document

@dataclass
class AtomRetrievalInfo:
    """åŸå­æ£€ç´¢ä¿¡æ¯æ•°æ®ç±»"""
    atom_query: str              # ç”¨æˆ·çš„æŸ¥è¯¢
    atom: str                    # åŒ¹é…åˆ°çš„åŸå­é—®é¢˜
    source_chunk_title: str      # æºæ–‡æ¡£æ ‡é¢˜
    source_chunk: str            # æºæ–‡æ¡£å®Œæ•´å†…å®¹
    source_chunk_id: str         # æºæ–‡æ¡£ID (å…³é”®å…³è”å­—æ®µ)
    retrieval_score: float       # æ£€ç´¢åˆ†æ•°
    atom_embedding: List[float]  # Atom å‘é‡


class ChunkAtomRetriever(BaseQaRetriever, ChromaMixin):
    """
    åŒå±‚å‘é‡å­˜å‚¨æ£€ç´¢å™¨
    
    æ ¸å¿ƒç»„ä»¶:
    - _chunk_store: Chunk å‘é‡å­˜å‚¨
    - _atom_store: Atom å‘é‡å­˜å‚¨
    
    å…¬å¼€æ¥å£:
    - retrieve_atom_info_through_atom(): é€šè¿‡ Atom æ£€ç´¢
    - retrieve_atom_info_through_chunk(): é€šè¿‡ Chunk æ£€ç´¢
    - retrieve_contents_by_query(): ç»¼åˆæ£€ç´¢
    - retrieve_contents(): ç­‰ä»·äº retrieve_contents_by_query(qa.question)
    """
    
    name: str = "ChunkAtomRetriever"
    
    def __init__(self, retriever_config: dict, log_dir: str, main_logger):
        super().__init__(retriever_config, log_dir, main_logger)
        
        # åŠ è½½åŒå‘é‡å­˜å‚¨
        self._load_vector_store()
        
        # åˆå§‹åŒ– Chroma æ··å…¥
        self._init_chroma_mixin()
        
        # è®¾ç½® Atom æ£€ç´¢çš„ k å€¼
        self.atom_retrieve_k = retriever_config.get(
            "atom_retrieve_k", 
            self.retrieve_k
        )
    
    def _load_vector_store(self):
        """åŠ è½½ Chunk Store å’Œ Atom Store"""
        vector_store_config = self._retriever_config["vector_store"]
        
        # é›†åˆåç§°
        collection_name = vector_store_config.get("collection_name", self.name)
        doc_collection_name = vector_store_config.get(
            "collection_name_doc", 
            f"{collection_name}_doc"
        )
        atom_collection_name = vector_store_config.get(
            "collection_name_atom", 
            f"{collection_name}_atom"
        )
        
        # æŒä¹…åŒ–ç›®å½•
        persist_directory = vector_store_config.get(
            "persist_directory", 
            self._log_dir
        )
        exist_ok = vector_store_config.get("exist_ok", True)
        
        # åŠ è½½ Embedding å‡½æ•°
        embedding_config = vector_store_config.get("embedding_setting", {})
        self.embedding_func = load_embedding_func(
            module_path=embedding_config.get("module_path"),
            class_name=embedding_config.get("class_name"),
            **embedding_config.get("args", {})
        )
        
        # ç›¸ä¼¼åº¦å‡½æ•° (ä½™å¼¦ç›¸ä¼¼åº¦)
        self.similarity_func = lambda x, y: (
            np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))
        )
        
        # åŠ è½½ Chunk Store
        loading_configs = vector_store_config["id_document_loading"]
        doc_ids, docs = load_callable(
            module_path=loading_configs["module_path"],
            name=loading_configs["func_name"],
        )(**loading_configs.get("args", {}))
        
        self._chunk_store = load_vector_store(
            collection_name=doc_collection_name,
            persist_directory=persist_directory,
            embedding=self.embedding_func,
            documents=docs,
            ids=doc_ids,
            exist_ok=exist_ok
        )
        
        # åŠ è½½ Atom Store
        loading_configs = vector_store_config["id_atom_loading"]
        atom_ids, atoms = load_callable(
            module_path=loading_configs["module_path"],
            name=loading_configs["func_name"],
        )(**loading_configs.get("args", {}))
        
        self._atom_store = load_vector_store(
            collection_name=atom_collection_name,
            persist_directory=persist_directory,
            embedding=self.embedding_func,
            documents=atoms,
            ids=atom_ids,
            exist_ok=exist_ok
        )
    
    # ... (retrieve æ–¹æ³•è§å‰æ–‡è¯¦è§£)
```

### 7.3 NetworkxMixin å®ç°

**æ–‡ä»¶**: `pikerag/knowledge_retrievers/mixins/networkx_mixin.py`

```python
from typing import Iterable
import networkx as nx

class NetworkxMixin:
    """
    NetworkX å›¾éå†æ··å…¥ç±»
    
    æä¾›åŸºäºå›¾ç»“æ„çš„çŸ¥è¯†æ‰©å±•èƒ½åŠ›
    """
    
    def _init_networkx_mixin(self):
        """åˆå§‹åŒ–å›¾éå†å‚æ•°"""
        self.entity_neighbor_layer: int = self._retriever_config.get(
            "entity_neighbor_layer", 
            1  # é»˜è®¤æ‰©å±• 1 è·³é‚»å±…
        )
    
    def _get_subgraph_by_entity(
        self, 
        graph: nx.Graph, 
        entities: Iterable, 
        neighbor_layer: int = None
    ) -> nx.Graph:
        """
        æ ¹æ®å®ä½“æå–å­å›¾
        
        ç®—æ³•:
        1. ä»ç»™å®šå®ä½“é›†åˆå¼€å§‹
        2. è¿­ä»£æ‰©å±• neighbor_layer å±‚é‚»å±…
        3. è¿”å›åŒ…å«æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹çš„å­å›¾
        
        Args:
            graph: å®Œæ•´çŸ¥è¯†å›¾è°±
            entities: èµ·å§‹å®ä½“é›†åˆ
            neighbor_layer: æ‰©å±•å±‚æ•°
        
        Returns:
            è¿‡æ»¤åçš„å­å›¾
        
        ç¤ºä¾‹:
            graph: A -- B -- C -- D
                   |
                   E
            
            entities = [A]
            neighbor_layer = 1
            â†’ subgraph nodes = {A, B, E}
            
            neighbor_layer = 2
            â†’ subgraph nodes = {A, B, C, E}
        """
        if neighbor_layer is None:
            neighbor_layer = self.entity_neighbor_layer
        
        # åˆå§‹åŒ–å®ä½“é›†åˆ
        entity_set = set(entities)
        newly_added = entity_set.copy()
        
        # è¿­ä»£æ‰©å±•é‚»å±…
        for layer in range(neighbor_layer):
            tmp_set = set()
            
            # éå†å½“å‰å±‚çš„æ‰€æœ‰èŠ‚ç‚¹
            for entity in newly_added:
                # è·å–é‚»å±…èŠ‚ç‚¹
                for neighbor in graph.neighbors(entity):
                    if neighbor not in entity_set:
                        tmp_set.add(neighbor)
            
            # æ›´æ–°
            newly_added = tmp_set
            entity_set.update(newly_added)
        
        # è¿”å›å­å›¾
        return graph.subgraph(nodes=entity_set)
```

**ä½¿ç”¨åœºæ™¯ç¤ºä¾‹**:

```python
# æ„å»ºçŸ¥è¯†å›¾è°±
import networkx as nx

# åˆ›å»ºå›¾: Chunk å’Œ Atom ä½œä¸ºèŠ‚ç‚¹
G = nx.Graph()

# æ·»åŠ  Chunk èŠ‚ç‚¹
G.add_node("chunk_001", type="chunk", content="...")
G.add_node("chunk_002", type="chunk", content="...")

# æ·»åŠ  Atom èŠ‚ç‚¹
G.add_node("atom_001", type="atom", question="...")
G.add_node("atom_002", type="atom", question="...")
G.add_node("atom_003", type="atom", question="...")

# æ·»åŠ è¾¹: Atom -> Chunk
G.add_edge("atom_001", "chunk_001")  # atom_001 æ¥æºäº chunk_001
G.add_edge("atom_002", "chunk_001")
G.add_edge("atom_003", "chunk_002")

# æ·»åŠ è¾¹: Chunk -> Chunk (ç›¸å…³æ–‡æ¡£)
G.add_edge("chunk_001", "chunk_002")  # ä¸¤ä¸ªæ–‡æ¡£ç›¸å…³

# ä½¿ç”¨ NetworkxMixin æå–å­å›¾
mixin = NetworkxMixin()
mixin.entity_neighbor_layer = 2

# ä»ä¸€ä¸ª Atom å‡ºå‘ï¼Œæ‰¾åˆ°ç›¸å…³çš„æ‰€æœ‰çŸ¥è¯†
subgraph = mixin._get_subgraph_by_entity(
    graph=G,
    entities=["atom_001"],
    neighbor_layer=2
)

print(subgraph.nodes())
# è¾“å‡º: ['atom_001', 'chunk_001', 'atom_002', 'chunk_002', 'atom_003']
# è§£é‡Š: ä» atom_001 å‡ºå‘ï¼Œ1 è·³åˆ° chunk_001ï¼Œ2 è·³åˆ°ç›¸å…³çš„å…¶ä»– atoms å’Œ chunks
```

---

## 8. é…ç½®ç¤ºä¾‹

### 8.1 å®Œæ•´çš„é—®é¢˜åˆ†è§£å·¥ä½œæµé…ç½®

**æ–‡ä»¶**: `examples/hotpotqa/configs/atomic_decompose.yml`

```yaml
# ============================================================
# å®éªŒè®¾ç½®
# ============================================================
experiment_name: atomic_decompose
log_root_dir: logs/hotpotqa
test_jsonl_filename: null
test_rounds: 1

# ============================================================
# å·¥ä½œæµè®¾ç½®
# ============================================================
workflow:
  module_path: pikerag.workflows.qa_decompose
  class_name: QaDecompositionWorkflow
  args:
    max_num_question: 5                    # æœ€å¤§åˆ†è§£é—®é¢˜æ•°
    question_similarity_threshold: 0.999   # é—®é¢˜å»é‡é˜ˆå€¼

# ============================================================
# æµ‹è¯•æ•°æ®åŠ è½½
# ============================================================
test_loading:
  module: pikerag.utils.data_protocol_utils
  name: load_testing_suite
  args:
    filepath: data/hotpotqa/dev_500.jsonl

# ============================================================
# Prompt åè®®è®¾ç½®
# ============================================================
decompose_proposal_protocol:
  module_path: pikerag.prompts.decomposition
  protocol_name: question_decompose_protocol

selection_protocol:
  module_path: pikerag.prompts.decomposition
  protocol_name: atom_question_selection_protocol

backup_selection_protocol:
  module_path: pikerag.prompts.decomposition
  protocol_name: chunk_selection_protocol

original_question_answering_protocol:
  module_path: pikerag.prompts.decomposition
  protocol_name: final_qa_protocol

# ============================================================
# LLM å®¢æˆ·ç«¯è®¾ç½®
# ============================================================
llm_client:
  module_path: pikerag.llm_client
  class_name: AzureOpenAIClient
  args: {}
  
  llm_config:
    model: gpt-4
    temperature: 0
  
  cache_config:
    location_prefix: null  # ä½¿ç”¨ experiment_name
    auto_dump: True

# ============================================================
# æ£€ç´¢å™¨è®¾ç½® (æ ¸å¿ƒé…ç½®)
# ============================================================
retriever:
  module_path: pikerag.knowledge_retrievers
  class_name: ChunkAtomRetriever
  
  args:
    # Chunk æ£€ç´¢å‚æ•°
    retrieve_k: 8                        # æ¯æ¬¡æ£€ç´¢è¿”å›çš„ Chunk æ•°é‡
    retrieve_score_threshold: 0.5        # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
    
    # Atom æ£€ç´¢å‚æ•°
    atom_retrieve_k: 4                   # æ¯æ¬¡æ£€ç´¢è¿”å›çš„ Atom æ•°é‡
    
    # å‘é‡å­˜å‚¨é…ç½®
    vector_store:
      # é›†åˆåç§°
      collection_name: dev_500_atomic_decompose_ada
      
      # æŒä¹…åŒ–ç›®å½•
      persist_directory: data/vector_stores/hotpotqa
      
      # Chunk æ•°æ®åŠ è½½
      id_document_loading:
        module_path: pikerag.utils.data_protocol_utils
        func_name: load_ids_and_chunks
        args:
          filepath: data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl
          atom_tag: atom_questions
      
      # Atom æ•°æ®åŠ è½½
      id_atom_loading:
        module_path: pikerag.utils.data_protocol_utils
        func_name: load_ids_and_atoms
        args:
          filepath: data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl
          atom_tag: atom_questions
      
      # Embedding è®¾ç½®
      embedding_setting:
        module_path: pikerag.llm_client.azure_open_ai_client
        class_name: AzureOpenAIEmbedding
        args: {}

# ============================================================
# è¯„ä¼°å™¨è®¾ç½®
# ============================================================
evaluator:
  metrics:
    - ExactMatch
    - F1
    - Precision
    - Recall
    - LLM
```

### 8.2 Tagging é…ç½®ç¤ºä¾‹

**æ–‡ä»¶**: `examples/hotpotqa/configs/tagging.yml`

```yaml
# ============================================================
# å®éªŒè®¾ç½®
# ============================================================
experiment_name: hotpotqa_dev_500
log_root_dir: logs/atom_tagging

# ============================================================
# æ–‡æ¡£åŠ è½½ä¸ä¿å­˜
# ============================================================
ori_doc_loading:
  module: pikerag.utils.data_protocol_utils
  name: load_chunks_from_jsonl
  args:
    jsonl_chunk_path: data/hotpotqa/dev_500_retrieval_contexts_as_chunks.jsonl

tagged_doc_saving:
  module: pikerag.utils.data_protocol_utils
  name: save_chunks_to_jsonl
  args:
    dump_path: data/hotpotqa/dev_500_retrieval_contexts_as_chunks_with_atom_questions.jsonl

# ============================================================
# Tagger è®¾ç½®
# ============================================================
tagger:
  tagging_protocol:
    module_path: pikerag.prompts.tagging
    attr_name: atom_question_tagging_protocol
  
  tag_name: atom_questions
  
  num_parallel: 1  # å¹¶è¡Œå¤„ç†æ•°é‡

# ============================================================
# LLM è®¾ç½®
# ============================================================
llm_client:
  module_path: pikerag.llm_client
  class_name: AzureOpenAIClient
  args: {}
  
  llm_config:
    model: gpt-4
    temperature: 0.7  # è¾ƒé«˜æ¸©åº¦ä»¥å¢åŠ é—®é¢˜å¤šæ ·æ€§
  
  cache_config:
    location_prefix: null
    auto_dump: True
```

---

## 9. æ€»ç»“ä¸æœ€ä½³å®è·µ

### 9.1 PIKE-RAG çŸ¥è¯†å›¾è°±çš„æ ¸å¿ƒåˆ›æ–°

1. **å¼‚æ„åŒå±‚ç»“æ„**
   - Chunk å±‚: ä¿è¯ä¸Šä¸‹æ–‡å®Œæ•´æ€§
   - Atom å±‚: æé«˜æ£€ç´¢ç²¾ç¡®åº¦
   - ä¸¤å±‚é€šè¿‡ `source_chunk_id` å…³è”

2. **è¯­ä¹‰å¯¹é½è®¾è®¡**
   - Atom ä»¥é—®é¢˜å½¢å¼è¡¨ç¤º
   - ä¸ç”¨æˆ·é—®é¢˜åœ¨è¯­ä¹‰ç©ºé—´å¤©ç„¶å¯¹é½
   - æé«˜æ£€ç´¢å¬å›ç‡

3. **çµæ´»çš„æ£€ç´¢ç­–ç•¥**
   - through_atom: é«˜ç²¾åº¦æ£€ç´¢
   - through_chunk: ä¸Šä¸‹æ–‡ä¸°å¯Œ
   - ç»¼åˆæ£€ç´¢: å¹³è¡¡ç²¾åº¦å’Œè¦†ç›–

4. **å¯æ‰©å±•çš„å›¾ç»“æ„**
   - NetworkxMixin æä¾›å›¾éå†èƒ½åŠ›
   - æ”¯æŒå¤šè·³æ¨ç†å’Œå…³è”å‘ç°

### 9.2 æœ€ä½³å®è·µ

#### 1. æ–‡æ¡£åˆ‡åˆ† (Chunking)

**æ¨èå‚æ•°**:
```yaml
chunk_size: 800-1200        # æ ¹æ®é¢†åŸŸè°ƒæ•´
chunk_overlap: 150-250      # ä¿è¯çŸ¥è¯†ä¸ä¸¢å¤±
```

**æ³¨æ„äº‹é¡¹**:
- ä¿æŒ Chunk è¯­ä¹‰å®Œæ•´æ€§
- é¿å…åœ¨å¥å­ä¸­é—´åˆ‡åˆ†
- ä½¿ç”¨ LLM è¾…åŠ©çš„æ™ºèƒ½åˆ‡åˆ†

#### 2. åŸå­é—®é¢˜æå– (Tagging)

**Prompt è®¾è®¡è¦ç‚¹**:
```
âœ“ è¦æ±‚å¤šæ ·æ€§: "extract as many questions as possible"
âœ“ é¿å…ä»£è¯: "avoid pronouns like it, he, she"
âœ“ åŒ…å«å®ä½“: "contain necessary entity names"
âœ“ æ ¼å¼æ˜ç¡®: "output line by line"
```

**è´¨é‡æ§åˆ¶**:
- æ¯ä¸ª Chunk æå– 3-7 ä¸ª Atoms
- å»é‡ç›¸ä¼¼é—®é¢˜
- äººå·¥æŠ½æŸ¥è´¨é‡

#### 3. æ£€ç´¢å‚æ•°è°ƒä¼˜

**é€šç”¨è®¾ç½®**:
```yaml
retrieve_k: 4-8             # æ ¹æ®ä»»åŠ¡å¤æ‚åº¦
atom_retrieve_k: 2-4        # é€šå¸¸å°äº retrieve_k
retrieve_score_threshold: 0.3-0.6  # é¿å…å™ªå£°
```

**åœºæ™¯ç‰¹å®š**:
- ç®€å•é—®ç­”: retrieve_k=4, atom_retrieve_k=2
- å¤šè·³æ¨ç†: retrieve_k=8, atom_retrieve_k=4
- æ¢ç´¢æ€§é—®é¢˜: retrieve_k=10, atom_retrieve_k=5

#### 4. æ€§èƒ½ä¼˜åŒ–

**å‘é‡åŒ–ä¼˜åŒ–**:
- ä½¿ç”¨æ‰¹é‡ embedding
- ç¼“å­˜å¸¸ç”¨æŸ¥è¯¢
- é€‰æ‹©åˆé€‚çš„ embedding æ¨¡å‹ (ada-002, bge-large ç­‰)

**å­˜å‚¨ä¼˜åŒ–**:
- å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
- ä½¿ç”¨ SSD å­˜å‚¨å‘é‡æ•°æ®åº“
- è€ƒè™‘åˆ†å¸ƒå¼éƒ¨ç½² (å¤§è§„æ¨¡åœºæ™¯)

**æ£€ç´¢ä¼˜åŒ–**:
- ä½¿ç”¨ approximate nearest neighbor (ANN)
- é¢„å…ˆè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„æ–‡æ¡£
- å¹¶è¡Œæ£€ç´¢ Chunk Store å’Œ Atom Store

### 9.3 å¸¸è§é—®é¢˜ (FAQ)

**Q1: ä¸ºä»€ä¹ˆä¸ä½¿ç”¨ä¼ ç»Ÿçš„å®ä½“-å…³ç³»å›¾è°±ï¼Ÿ**

A: PIKE-RAG çš„å¼‚æ„çŸ¥è¯†å›¾è°±æ›´é€‚åˆæ–‡æ¡£é—®ç­”åœºæ™¯:
- ä¸éœ€è¦é¢„å®šä¹‰ schema
- LLM è‡ªåŠ¨æå–çŸ¥è¯†ç‚¹
- é¿å…å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–çš„è¯¯å·®
- æ›´çµæ´»ï¼Œé€‚åº”å¤šæ ·åŒ–çš„æŸ¥è¯¢

**Q2: Atom å’Œ Chunk çš„æ¯”ä¾‹åº”è¯¥æ˜¯å¤šå°‘ï¼Ÿ**

A: æ¨è Atom:Chunk = 3:1 åˆ° 7:1
- æ¯”ä¾‹å¤ªä½: æ£€ç´¢ç²¾åº¦ä¸è¶³
- æ¯”ä¾‹å¤ªé«˜: å¢åŠ å­˜å‚¨å’Œè®¡ç®—æˆæœ¬
- æ ¹æ®æ–‡æ¡£å¯†åº¦è°ƒæ•´

**Q3: å¦‚ä½•å¤„ç†å¤šè¯­è¨€åœºæ™¯ï¼Ÿ**

A:
- ä½¿ç”¨å¤šè¯­è¨€ Embedding æ¨¡å‹ (å¦‚ multilingual-e5)
- åˆ†åˆ«ä¸ºæ¯ç§è¯­è¨€æå– Atoms
- åœ¨ metadata ä¸­æ ‡è®°è¯­è¨€

**Q4: çŸ¥è¯†å›¾è°±éœ€è¦å¤šä¹…æ›´æ–°ä¸€æ¬¡ï¼Ÿ**

A: æ ¹æ®æ•°æ®å˜åŒ–é¢‘ç‡:
- é™æ€çŸ¥è¯†åº“: ä¸€æ¬¡æ€§æ„å»º
- å‘¨æ›´æ–°: å¢é‡æ›´æ–° (æ·»åŠ æ–° Chunks å’Œ Atoms)
- å®æ—¶æ›´æ–°: æµå¼å¤„ç† + å®šæœŸé‡å»ºç´¢å¼•

**Q5: å¦‚ä½•è¯„ä¼°çŸ¥è¯†å›¾è°±è´¨é‡ï¼Ÿ**

A: å…³é”®æŒ‡æ ‡:
- Atom è´¨é‡: äººå·¥è¯„ä¼°ç›¸å…³æ€§å’Œå¤šæ ·æ€§
- æ£€ç´¢æ•ˆæœ: Recall@K, MRR
- ç«¯åˆ°ç«¯: QA ä»»åŠ¡çš„ F1/EM åˆ†æ•°

### 9.4 è¿›é˜¶åº”ç”¨

#### 1. å¤šè·³æ¨ç†å¢å¼º

```python
# ä½¿ç”¨ NetworkxMixin æ„å»ºæ˜¾å¼å›¾
G = nx.Graph()

# æ·»åŠ  Chunk-Atom å…³ç³»
for chunk_id, atoms in chunks_with_atoms:
    G.add_node(chunk_id, type="chunk")
    for atom in atoms:
        atom_id = hash(atom)
        G.add_node(atom_id, type="atom", question=atom)
        G.add_edge(atom_id, chunk_id)

# æ·»åŠ  Chunk-Chunk ç›¸ä¼¼åº¦è¾¹
for chunk1, chunk2, similarity in chunk_similarities:
    if similarity > threshold:
        G.add_edge(chunk1, chunk2, weight=similarity)

# å¤šè·³æ£€ç´¢
start_atoms = retriever.retrieve_atom_info_through_atom(query)
start_chunk_ids = [info.source_chunk_id for info in start_atoms]

# æ‰©å±• 2 è·³é‚»å±…
subgraph = mixin._get_subgraph_by_entity(G, start_chunk_ids, neighbor_layer=2)
expanded_chunks = [n for n in subgraph.nodes() if G.nodes[n]["type"] == "chunk"]
```

#### 2. çŸ¥è¯†å›¾è°±å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import networkx as nx

# ç»˜åˆ¶å­å›¾
pos = nx.spring_layout(subgraph)
nx.draw(subgraph, pos, with_labels=True, node_color='lightblue')
plt.show()
```

#### 3. çŸ¥è¯†è’¸é¦

å°† LLM æå–çš„ Atoms è’¸é¦ä¸ºå°æ¨¡å‹:
```python
# ä½¿ç”¨ Atoms ä½œä¸ºè®­ç»ƒæ•°æ®
train_data = [
    {"question": atom, "context": chunk, "answer": extract_answer(atom, chunk)}
    for chunk_id, atoms in tagged_chunks
    for atom in atoms
]

# å¾®è°ƒå°æ¨¡å‹
model.train(train_data)
```

---

## ğŸ“š å‚è€ƒèµ„æº

### ä»£ç æ–‡ä»¶ç´¢å¼•

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ |
|------|---------|
| Chunking å·¥ä½œæµ | `pikerag/workflows/chunking.py` |
| Tagging å·¥ä½œæµ | `pikerag/workflows/tagging.py` |
| ChunkAtomRetriever | `pikerag/knowledge_retrievers/chunk_atom_retriever.py` |
| NetworkxMixin | `pikerag/knowledge_retrievers/mixins/networkx_mixin.py` |
| Atom Tagging Prompt | `pikerag/prompts/tagging/atom_question_tagging.py` |
| æ•°æ®åŠ è½½å·¥å…· | `pikerag/utils/data_protocol_utils.py` |
| QA å·¥ä½œæµ | `pikerag/workflows/qa.py` |
| QA åˆ†è§£å·¥ä½œæµ | `pikerag/workflows/qa_decompose.py` |

### é…ç½®æ–‡ä»¶ç´¢å¼•

| ç”¨é€” | æ–‡ä»¶è·¯å¾„ |
|------|---------|
| Chunking é…ç½® | `examples/biology/configs/chunking.yml` |
| Tagging é…ç½® | `examples/hotpotqa/configs/tagging.yml` |
| QA é…ç½® | `examples/hotpotqa/configs/atomic_decompose.yml` |
| Retriever æ¨¡æ¿ | `pikerag/knowledge_retrievers/templates/ChunkAtomRetriever.yml` |

### ç›¸å…³æ–‡æ¡£

- [åŸºç¡€ QA å·¥ä½œæµè¯¦è§£](./åŸºç¡€QAå·¥ä½œæµè¯¦è§£.md)
- [ChunkAtomRetriever è¯¦è§£](./ChunkAtomRetrieverè¯¦è§£.md)
- [æ–‡æ¡£å¤„ç†ä¸æ™ºèƒ½åˆ‡åˆ†è¯¦è§£](./æ–‡æ¡£å¤„ç†ä¸æ™ºèƒ½åˆ‡åˆ†è¯¦è§£.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**æœ€åæ›´æ–°**: 2024å¹´  
**ä½œè€…**: PIKE-RAG Team

